%\VignetteIndexEntry{stratEst_vignette_0.1.2}
%\VignetteEngine{R.rsp::tex}
%\VignetteKeyword{R}
%\VignetteKeyword{package}
%\VignetteKeyword{vignette}
%\VignetteKeyword{LaTeX}

\documentclass[12pt,letter]{article}
\usepackage[utf8]{inputenc}
\usepackage{fontenc}
\usepackage[a4paper]{geometry}
\geometry{width=22cm, left=2cm, top=3cm, bottom=3cm}
\usepackage[english]{babel}
\usepackage{natbib}
\usepackage{booktabs}
\usepackage{setspace}
\onehalfspacing
\usepackage{tabulary}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usepackage{amsmath}
\usepackage{amssymb}

\begin{document}
%\begin{small}
\newgeometry{right=2cm,left=2cm, bottom=3cm, top=3.5cm}
\begin{titlepage}
\begin{spacing}{0.9}

\title{\LARGE \textsf{stratEst}: \textsc{Strategy Estimation in} \textsf{R}\footnote{Please cite this document as: Dvorak, Fabian (2019). stratEst: Strategy Estimation in R. R package vignette version 0.1.2. https://CRAN.R-project.org/package=stratEst. I would like to thank Yongping Bao, Yves Breitmoser, Karsten Donnay, Urs Fischbacher, Konstantin K\"appner and Susumu Shikano for helpful comments. I am particularly grateful to Sebastian Fehrler for many inspiring conversations. All remaining errors are my own.}\vspace{10pt}}
%\runningheads{}{Negotiating Cooperation under Uncertainty}
\author{\large{Fabian Dvorak}\vspace{2pt}\\ \normalsize{University of Konstanz} \vspace{2pt}\\ \small{fabian.dvorak@uni.kn}
}

\date{\vspace{10pt} \small December 2018}

\maketitle
\thispagestyle{empty}

\vspace{-0.15in}

\begin{abstract}
\noindent 
\textsf{stratEst} is a statistical software package which can be used to characterize the choices of a sample of individuals as a mixture of individual strategies. Strategies can be estimated from the data or supplied by the user in the form of deterministic finite-state automata. The package uses the EM algorithm \citep{Dempster1977} and the Newton-Raphson method to obtain maximum-likelihood estimates of the population shares and choice parameters of the strategies. The number and the complexity of strategies can be restricted by the user or selected based on information criteria. The package also features an extension of strategy estimation in the spirit of latent class regression to assess the effects of covariates on strategy use.
\end{abstract}
\end{spacing}
\vspace{25pt}
\noindent
\textbf{Keywords:} experimental games, individual differences, mixture models \vspace{10pt} \\
\noindent 
\textbf{JEL Classification:} C13, C87, C91, C92\vspace{3cm}
\end{titlepage}

\newgeometry{right=2.25cm,left=2.25cm, bottom=3.2cm, top=3.2cm}

\section{Introduction}
\setlength\parindent{0pt}
%\begin{quote}
%\textit{In the past half century, economics has been enriched by vast new resources of microeconomic data. These data have opened the eyes of economists to the diversity and heterogeneity of economic life.} \citep[][p. 734]{Heckman2001}
%\end{quote}
%\noindent
%The quote from the Nobel Lecture of James J. Heckman describes an important empirical regularity of economic behavior. Heterogeneous behavior is the norm in most economic environments. However, most theoretical work in economics still assumes that agents are homogeneous, often because the heterogeneity in behavior not well understood. Laboratory experiments offer the opportunity to study heterogeneity in behavior under controlled laboratory conditions. A prerequisite for the analysis of behavioral heterogeneity in economic experiments is statistical methods that can be used to analyze differences in strategy choice.

%What is it?
\textsf{stratEst} is a software package for the  statistical computing environment \textsf{R} (R Development Core Team, \citeyear{R2008}). The package implements variants of the strategy estimation method \citep{DalBo2011}. The goal of strategy estimation is to characterize the choices of a sample of individuals as a mixture of individual strategies. Strategy estimation is similar to mixture modeling \citep{McLachlan2005}, cluster analysis \citep{Kaufman1990}, and latent class analysis \citep{Lazarsfeld1950}. All three methods essentially group several entities into several unobservable classes. In the case of strategy estimation, the entities are individuals and the unobservable classes are strategies in the sense of game-theory. A strategy is a complete action plan which prescribe a behavioral response for every situation in a game.\footnote{Researchers not interested in social interaction may think of a strategy as a behavioral algorithm for a specific decision-making environment instead.} 

Different variants of strategy estimation exist \citep{DalBo2011,Breitmoser2015,Dvorak2018b}. In one variant, theory indicates a set of reasonable candidate strategies and the researcher is interested to estimate the frequency of the strategies in the population given the sampled data. In another variant, reasonable candidate strategies are unknown and the researcher wants to learn something about the the choice parameters of the strategies while imposing some basic assumptions on the number and the general structure of the strategies. In yet another variant, the researcher is interested in how strategy use is influenced by covariates such as treatment conditions or time. 

% Main solution
The main challenge for strategy estimation software is to guarantee a sufficient degree of flexibility across the different variants of the method for large number of games. To address this issue, the \textsf{stratEst} package handles strategies in the form of deterministic finite-state automata.\footnote{The strategy estimation variants two and three can in principle also be conducted based on \textsf{R} packages for cluster analysis like Flexmix \citep{Leisch2004}, poLCA \citep{Linzer2011} and randomLCA \citep{Beath2011}. The disadvantage of (mis)using these packages for strategy estimation is that all candidate strategies must have the same simple structure and cannot be reasonably adapted to the game or the decision-making environment.}  Deterministic finite-state automata map all possible situations of a game into a finite set of strategy-specific states. The behavioral response of an individual following a certain strategy is then a function of the the strategy-specific state and not the situation itself. The handling of strategies as deterministic finite-state automata offers a concise way to customize strategies for many different games and even works for games with infinitely many situations.

% Limitations and implementation
The current version of the package is limited to the estimation of strategies which prescribe discrete choices. The central modeling assumption is that the observed actions are independent draws from a multinomial distribution with parameters defined by the current state of the strategy used. Maximum-likelihood estimates for the model parameters are obatined based on variants of the Expectation-Maximization algorithm \citep{Dempster1977} and the Newton-Raphson method. To increase speed the estimation procedures, \textsf{stratEst} uses integration of C++ and \textsf{R} through \textsf{Rcpp} \citep{Eddelbuettel2011} as well as the open source linear algebra library for the C++ language \textsf{RppArmadillo} \citep{Sanderson2016}. Package development is supported by the packages \textsf{devtools} \citep{Wickham2018a},  \textsf{testthat} \citep{Wickham2011}, \textsf{roxygen2} \citep{Wickham2018b}, \textsf{knitr} \citep{Xie2018}, and \textsf{R.rsp} \citep{Bengtsson2018}.

% Roadmap of the vignette
The introduction continues with information on how to install the package and two examples. Section \ref{sec: Deterministic finite-state automata} of the package vignette illustrates how strategies are represented as deterministic finite-state automata. Section \ref{sec: Strategy estimation} introduces the strategy estimation method. The general model and algorithm that is used to obtain maximum-likelihood estimates of the model parameters is introduced. Section \ref{sec:Model selection} covers model selection. It explains how the number of model parameters can be restricted by the user or selected based on information criteria. Section \ref{sec:Latent class regression} introduces the extension of the strategy estimation method in the spirit of latent class regression to assess the role of covariates for strategy use. Section \ref{sec:Standard errors} explains the estimation procedures for the standard errors of the model parameters. Section \ref{sec:simulation} illustrates the validity of the estimation procedures based on a simulation exercise. Section \ref{sec:Using stratEst} gives an overview over the syntax of the estimation function and its input and output objects.
\subsection*{Installation}
The most recent CRAN version of the \textsf{stratEst}
package can be installed by executing the following code in the R console:\bigskip\\
\texttt{install.packages("stratEst")}\bigskip\\
You can also install the most recent development version of the package from GitHub using the \textsf{devtools} package:\bigskip\\
\texttt{install.packages("devtools")}\\
\texttt{library(devtools)}\\
\texttt{install$\_$github("fdvorak/stratEst")}
\bigskip\\
After successful installation, the package is loaded into memory and attached to the search path in the usual way by:\bigskip\\
\texttt{library(stratEst)}\bigskip\\
Now the package is ready to use.
\subsection*{An introductory example}
\label{sec: helping game}
In this example, the \textsf{stratEst} package is used to perform strategy estimation based on the fictitious data depicted in Figure \ref{fig:example data}. It will be useful to assume that the data set contains the data of two individuals playing four periods of a helping game with each other. In each period, both individuals simultaneously decide to help the other individual or not. Helping is costly but receiving help implies a monetary benefit which exceeds the costs of helping. Each row of the data shown in Figure \ref{fig:example data} represents the action of one individual in one period of the game. The first four columns identify the individual, the pairwise matching of individuals, the game, and the period within the game. The fifth column contains a dummy variable which is one if the individual helped the other individual in the respective period and zero otherwise.  
\begin{figure}[htbp]
\caption{Fictitious data of a helping game}
\label{fig:example data}
\begin{equation*}
\begin{matrix} 
id & group & game & period & help \\
62 & 13 & 4 & 1 & 1\\
62 & 13 & 4 & 2 & 1\\
62 & 13 & 4 & 3 & 0\\
62 & 13 & 4 & 4 & 1\\
87 & 13 & 4 & 1 & 1\\
87 & 13 & 4 & 2 & 0\\
87 & 13 & 4 & 3 & 1\\
87 & 13 & 4 & 4 & 0\\
\end{matrix} 
\end{equation*}
\end{figure}  

The goal of strategy estimation is to explain the actions of the two individuals based on strategies that take the behavior of the other individual into account. Two tasks have to be completed by the user before the estimation can be performed.

The first task is to define a common set of inputs and outputs for the candidate strategies. Inputs represent information that can be observed by a player at a certain stage of the game and potentially influence her continuation strategy. Outputs represent possible actions at a certain stage  of the game. If both individual can observe their actions, it is natural to define the action profile of the previous period as the new information the strategies react to on a regular basis. In the following, the action profiles $(help,help)$, $(help,no)$, $(no,help)$, $(no,no)$ will be represented by the input values 1, 2, 3, and 4 respectively. The input value 0 is special and must be used at the beginning of a game when no information about past play is available. For the outputs, the two possible actions $no~help$ and $help$ will be represented by the values 1 and 2 in the following. Top complete the first task, the data  depicted in Figure \ref{fig:example data} must be stored as an \textsf{R} data frame augmented by the variables \texttt{input} and \texttt{output} (see Subsection \ref{subsec:Input objects} for information on other data structures). This is achieved by executing the following statements in the \textsf{R} console:\bigskip\\
\texttt{id <- c(62,62,62,62,87,87,87,87) }\\
\texttt{game <- c(4,4,4,4,4,4,4,4) }\\
\texttt{period <- c(1,2,3,4,1,2,3,4)  }\\
\texttt{input <- c(0,1,2,3,0,1,3,2)  }\\
\texttt{output <- c(2,2,1,2,2,1,2,1) }\\
\texttt{data <- as.data.frame(cbind(id,game,period,input,output)) }\bigskip\\
The second task is to define which outputs follow after a each possible combination of inputs for each candidate strategy. In the following two candidate strategies are defined. One reciprocal strategy, that randomizes in the first period and subsequently helps if the other participant helped in the previous period, and one alternating strategy, that helps with probability 0.9 in uneven periods and with unknown probability in even periods.\footnote{Inspection of the data reveals, that the reciprocal strategy perfectly describes the behavior of participant 62 while the alternating strategy provides a better description of the behavior of participant 87. With more data, such inference will of course not be possible and it will be necessary to define reasonable candidate strategies based on theory.} Each of these two strategies can be represented by a deterministic finite-state automaton (see Section \ref{sec: Deterministic finite-state automata} for more information). The following code creates an \textsf{R} matrix which contains the deterministic finite-state automaton representations of the two strategies:\bigskip\\
\texttt{strategies <- matrix(c(1,2,3,1,2,0.5,0,1,0.1,NA,0.5,1,0,0.9,NA,2,2,2,2,1,\\
\hspace*{4.98cm}3,3,3,2,1,2,2,2,2,1,3,3,3,2,1),5,7)}\bigskip\\
Printed out in the console, the matrix looks like this:\medskip\\ 
\texttt{> strategies }\medskip\\
$\begin{matrix} 
 	 & \texttt{[,1]} & \texttt{[,2]} & \texttt{[,3]} & \texttt{[,4]} & \texttt{[,5]} & \texttt{[,6]} & \texttt{[,7]}\\ 
\texttt{[1,]} & \texttt{1} & \texttt{0.5} & \texttt{0.5} & \texttt{2} & \texttt{3} & \texttt{2} & \texttt{3}\\
\texttt{[2,]} & \texttt{2} & \texttt{0.0} & \texttt{1.0} & \texttt{2} & \texttt{3} & \texttt{2} & \texttt{3}\\
\texttt{[3,]} & \texttt{3} & \texttt{1.0} & \texttt{0.0} & \texttt{2} & \texttt{3} & \texttt{2} & \texttt{3}\\
\texttt{[4,]} & \texttt{1} & \texttt{0.1} & \texttt{0.9} & \texttt{2} & \texttt{2} & \texttt{2} & \texttt{2}\\
\texttt{[5,]} & \texttt{2} & \texttt{NA} & \texttt{NA} & \texttt{1} & \texttt{1} & \texttt{1} & \texttt{1}\\
\end{matrix}$
\bigskip\\
Each row of the matrix represents one state of an automaton. The first three rows of the matrix define the reciprocal strategy. The last two rows of the matrix define the alternating strategy. The first column of the matrix enumerates the states of within each automaton. A row with the value 1 in the first column represents the start state of the automaton. By definition, a strategy is in its start state whenever the current input is zero. The second column indicates the probability of the output which has the lowest output value for the respective state. In the example, the output with the lowest value is the one that represents $no~help$ and the probability to observe this action as output in the start state is one half for the reciprocal strategy. The third column indicates the probability of the output with the next higher value which represents the action $help$. The pattern according to which an automaton moves from one state to the next over the periods of the game is defined in columns four to seven. The value 2 in the first row of column four indicates that the reciprocal strategy moves from state 1 to state 2 if the input is 1. The value 3 in the first row of column five indicates that the reciprocal strategy moves from state 1 to state 3 if the input is 2, and so on.\footnote{Note that inputs were defined such that inputs 1 and 3 indicate help of the other participant in the previous period. As the reciprocal strategy moves to state 2 after these inputs, and helps with probability of one. The probability to help after observing input 2 or 4 which indicate no help, is zero as the strategy moves to state 3.} Rows four and five of the matrix \texttt{strategies} define the alternating strategy in a similar fashion. The fact that the probability to help in even periods is ex-ante unknown is indicated by inserting NA in column two.\footnote{Whenever NA is supplied for a model parameter, \textsf{stratEst} will estimate this parameter from the data. See Section \ref{sec: Strategy estimation} for information on the general model and its parameters.} 

The central function of the package is the function \texttt{stratEst()}. It is the estimation function which implements all variants of strategy estimation (see Section \ref{sec:Using stratEst} for more information). The following code is executed in the console to obtain maximum-likelihood estimates of the population shares and the choice parameters of the two candidate strategies:\bigskip\\
\texttt{model <- stratEst(data,strategies)}\bigskip\\ 
The output objects of the function are stored as an\textsf{R} object of type list and can be called by using the syntax \texttt{model\$object} where \texttt{object} can be one of the output objects (see Subsection \ref{subsec:Output objects}). To display the estimates of the population shares and the final strategy matrix the objects \texttt{model\$ shares} and \texttt{model\$strategies} can be used to display the following results:\bigskip\\
\begin{minipage}[t]{4cm}
\texttt{> model\$shares }\\
$\begin{matrix} 
 	 & \texttt{[,1]}\\ 
\texttt{[1,]} & \texttt{0.5} \\
\texttt{[2,]} & \texttt{0.5} \\
\end{matrix}$
\end{minipage}
\begin{minipage}[t]{9cm}
\texttt{> model\$strategies }\\
$\begin{matrix} 
 	 & \texttt{[,1]} & \texttt{[,2]} & \texttt{[,3]} & \texttt{[,4]} & \texttt{[,5]} & \texttt{[,6]} & \texttt{[,7]}\\ 
\texttt{[1,]} & \texttt{1} & \texttt{0.5} & \texttt{0.5} & \texttt{2} & \texttt{3} & \texttt{2} & \texttt{3}\\
\texttt{[2,]} & \texttt{2} & \texttt{0.0} & \texttt{1.0} & \texttt{2} & \texttt{3} & \texttt{2} & \texttt{3}\\
\texttt{[3,]} & \texttt{3} & \texttt{1.0} & \texttt{0.0} & \texttt{2} & \texttt{3} & \texttt{2} & \texttt{3}\\
\texttt{[4,]} & \texttt{1} & \texttt{0.1} & \texttt{0.9} & \texttt{2} & \texttt{2} & \texttt{2} & \texttt{2}\\
\texttt{[5,]} & \texttt{2} & \texttt{1.0} & \texttt{0.0} & \texttt{1} & \texttt{1} & \texttt{1} & \texttt{1}\\
\end{matrix}$
\end{minipage}
\bigskip\\
The first element of the vector  \texttt{model\$shares} indicates that the estimated share of individuals in the sample that use the first strategy defined in \texttt{strategies} is 50\%. The second element reveals that the estimated share of the second strategy is also 50\%. The matrix \texttt{model\$strategies} returns the strategies which correspond to the estimated population shares in \texttt{model\$shares}. The format of the matrix corresponds to the format of the input object \texttt{strategies}. The last element in the second column of the estimated strategies indicates that the ex-ante unknown probability to help in even periods when using the alternating strategy is estimated to be zero which coincides with the behavior of the second individual in period four.

\subsection*{Repeated prisoner's dilemma example}
\label{sec: A prisoner's dilemma example}
This example illustrates how to perform strategy estimation based on data from an indefinitely repeated prisoner's dilemma.\footnote{The indefinitely repeated prisoner's dilemma is special case in two ways: First, data can be submitted in a format specific to this game. Second, the package comes with a set of 22 pre-programmed strategies for the repeated prisoner's dilemma listed in Tables \ref{tab:PD_set1}-\ref{tab:PD_set3} of the Appendix.} It is shown how to replicate the results of \cite{DalBo2011} which was the first paper that performed strategy estimation. The paper reports results on the evolution of cooperation in the indefinitely repeated prisoner's dilemma across six different treatments. The six treatments differ in the stage-game parameters and the continuation probability $\delta$ of the repeated game. The stage-game parameters are depicted in Figure \ref{fig:PD} where the parameter $R$ is either 32, 40 or 48. For each value of $R$ two treatments exist with $\delta$ of $1/2$ or $3/4$ resulting in 2 times three between subject design with six treatments overall.
\begin{figure}[htbp]
\caption{Stage game of \cite{DalBo2011}}
\label{fig:PD}
\
\vspace*{0.2cm}
\centering
		\begin{tabular}{lcc}
			& C & D \\ [0.8ex]
			\cline{2-3}
			& \multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{}\\ [-0.5ex]
			C       &  \multicolumn{1}{|c|}{R,R} &  \multicolumn{1}{c|}{$12$,$50$}\\  [-1.25ex]
			& \multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{}\\
			\cline{2-3} 
			& \multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{}\\ [-0.5ex]
			D       &  \multicolumn{1}{|c|}{$50$,$12$} &  \multicolumn{1}{c|}{25,25}\\ [-1.25ex]
			& \multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{}\\
			\cline{2-3} \\[-0.8ex]
		\end{tabular}
\end{figure}
Figure \ref{fig:PD data} displays the first 8 rows of the data of the experiment conducted by \cite{DalBo2011}. The first column identifies the treatment. The second column contains an identifier of the participants. Column three enumerates the supergame as each participants plays many repeated games during the experiment. Column four indicates the period within the supergame. Column five and six contain dummy variables which indicate whether the participant and the other player cooperated in the current period of the supergame. 
\begin{figure}[htbp]
\caption{First ten rows of data from \cite{DalBo2011}}
\label{fig:PD data}
\begin{equation*}
\begin{matrix} 
treatment & id & supergame & period & cooperation & other\_cooperation \\
1 & 1 & 62 & 1 & 0 & 0 \\
1 & 1 & 63 & 1 & 0 & 0 \\
1 & 1 & 63 & 2 & 1 & 0 \\
1 & 1 & 63 & 3 & 0 & 1 \\
1 & 1 & 64 & 1 & 0 & 0 \\
1 & 1 & 64 & 2 & 1 & 0 \\
1 & 1 & 64 & 3 & 0 & 0 \\
%1 & 1 & 64 & 4 & 0 & 0 \\
%1 & 1 & 64 & 5 & 0 & 0 \\
%1 & 1 & 64 & 6 & 0 & 0 \\
\end{matrix} 
\end{equation*}
\end{figure}  
The data is available as an $R$ data frame and the first 8 rows can be inspected in the console by typing \texttt{DF2011[1:8,]}.\footnote{Note that the structure of the data is different to the structure explained in the previous example. Data from a repeated prisoner's dilemma can also be used in the form displayed in Figure \ref{fig:PD data}. The estimation function will assume that data is from a repeated prisoner's dilemma whenever this structure is used. Another possibility is to omit the variable \texttt{other\_cooperation} and instead use a group identifier with column name \texttt{group} which identifies the pairwise matching of participants for a supergame. See Subsection \ref{subsec:Input objects} more information.}

\cite{DalBo2011} report the results of treatment-wise strategy frequency estimation for six candidate strategies which are: Always Defect (ALLD), Always Cooperate (ALLC), Tit-For-Tat (TFT), Grim-Trigger (GRIM), Win-Stay-Lose-Shift (WSLS), and a trigger strategy with two periods of punishment (T2). The six strategies are included in a set of the pre-programmed repeated prisoner's dilemma listed in Tables \ref{tab:PD_set1}-\ref{tab:PD_set3} of the Appendix. The following code can be used to replicate the findings of \cite{DalBo2011}, where \texttt{treatment} $\in \{1,\cdots,6\}$ specifies the treatment number.\bigskip\\
\texttt{data <- DF2011[DF2011\$treatment == 1,]}\\
\texttt{strategies <- rbind(ALLD,ALLC,GRIM,TFT,WSLS,T2)}\\
\texttt{model <- stratEst(data,strategies)}\bigskip\\
The estimated population shares can be inspected with the command \texttt{model\$shares} and are identical to those reported in the first column of Table 7 on page 424 of \cite{DalBo2011}.

\section{Strategies as Deterministic Finite-State Automata}
\label{sec: Deterministic finite-state automata}
Candidate strategies can be customized by the user and handed over to the estimation function in the form of a deterministic finite-state automata (DFA). The DFA representation of a strategy groups all situations where a strategy prescribes an identical behavioral response into one state of the automaton. To give an example, consider the Tit-For-Tat (TFT) strategy which starts with cooperation and subsequently mimics the behavior of the other player in the previous period. TFT cooperates if the other player cooperated in the previous period and defects if the other player defected in the previous period. If a game continues for several periods, there are many different situations for which TFT prescribes cooperation. For example, in period three, if the action of the other player was defection in period one and cooperation in period two but also if the action of the other player was cooperation in both of the previous periods. Characterizing TFT by specifying the behavioral response in each possible situation is a daunting task if the game has many periods. 

The TFT strategy can be characterized by an automaton with only two states $C$ and $D$, and two corresponding multinomial response vectors $\pi_{C} = \{\pi^{c}_{C}=1,\pi^{d}_{C}=0\}$ and $\pi_{D} = \{\pi^{c}_{D}=0,\pi^{d}_{D}=1\}$. The elements of the response vectors define the probability for action $a \in \{c,d\}$ conditional on the state $s \in \{C,D\}$ of the TFT automaton. In state $C$ the automaton representation of TFT prescribes to play $c$. In state $D$ it prescribes to play $d$. At the same time, the TFT automaton performs deterministic state transitions conditional on the current state of the automaton and some input. In the case of TFT, the input is the action profile of the previous period. Let $a_{i}a-{j}$ denote the action profile of the previous period where $a_{i}$ indicates the own action and $a_{j}$ the action of the other player. In state $C$, TFT remains in state $C$ if the input is $cc$ or $dc$ and changes to state $D$ if the input is $dc$ or $dd$. In state $D$, TFT remains in state $D$ if the input is $dc$ or $dd$ and changes to state $C$ if the input is $c$ or $dc$. The input value 0 is used to indicate the empty action profile for period one.
\begin{figure}[htbp]
\caption{The Tit-For-Tat and Always-Defect automata}
\label{Fig:TFT}
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse,fill=blue!0, node distance=2cm,minimum height=2em]
\begin{center}
\begin{tikzpicture}[baseline=(C.base),node distance = 2cm, auto]
    % Place nodes
    \node [cloud] (C) {C};
    \node[fill=white] at (1,0.9) (Photo) {{\small $cd$,$dd$}};
    \node[fill=white] at (1,-0.9) (Photo) {{\small $cc$,$dc$}};
    \node[fill=white] at (-1.3,0) (Photo) {{\small $cc$,$dc$}};
    \node[fill=white] at (3.4,0) (Photo) {{\small $cd$,$dd$}};
    \node [cloud, right of=C] (D) {D};
    \draw [->] (C) to [out=45,in=135] (D);
    \draw [->] (C) to [out=-135,in=135,looseness=4] (C);
    \draw [->] (D) to [out=-45,in=45,looseness=4] (D);
        \draw [->] (D) to [out=-135,in=-45] (C);
   \end{tikzpicture}
   \hspace{1cm}
\begin{tikzpicture}[baseline=(C.base),node distance = 2cm, auto]
    % Place nodes
    \node [cloud] (D) {D};
    \node[fill=white] at (0,-1) (Photo) {};
    \draw [->] (D) to [out=225,in=315,looseness=4] (D);
   \end{tikzpicture}
\end{center}
\end{figure}

The two-state TFT automaton is depicted in the left panel of Figure \ref{Fig:TFT}. The two states are represented by the two nodes labeled $C$ and $D$. The deterministic state transitions of the TFT automaton are illustrated by the arrows leaving the nodes. The information next to an arrow indicates the inputs for which the transition takes place. 

To give an example of an even simpler automaton, the right panel of Figure \ref{Fig:TFT} depicts the DFA representation of the strategy which always defects (ALLD). ALLD is represented as a DFA with only one state $D$ with the response vector $\pi_{D} = \{ \pi^{c}_{D} = 0, \pi^{d}_{D} = 1\}$. Since the automaton has only one state, the deterministic state transition function points from state $D$ to itself for every possible input. The fact that this transition occurs for every possible input is indicated with an unlabeled arrow.
\subsection{General definition}
\textsf{stratEst} uses the following general definition of a deterministic finite-state automaton. For a game with inputs $\omega \in \Omega$ and discrete responses $r \in R$ the DFA $k \in K$ is a 4-tuple $(S_{k}, s_{k0},\phi_{k}, \pi_{k})$. $S_{k}$ is a finite set of strategy-specific states with elements $s$ and start state $s_{k0}  \in S$. $\phi_{k} : S_{k} \times \omega \rightarrow S_{k}$ is a deterministic transition function which maps every possible combination of states and inputs into the set of states. $\pi_{k}$ is a collection of multinomial response vectors. The collection contains one vector for each state with $R$ elements and $\sum_{r=1}^{R} \pi_{ksr} = 1, ~ \forall~s\in S_{k}$ and $k \in K$. The DFA $(S_{k}, s_{k0},\phi_{k}, \pi_{k})$ prescribes a probabilistic response pattern for every situation of the game. 

To give an example, if automaton $k$ is the TFT automaton: $S_{k} \in \{C,D\}$, $s_{k0} = C$, $\phi_{k}(s,c) = C$ and $\phi_{k}(s,d) = D,~\forall~s \in \{C,D\}$, $\pi_{ks} = \{\pi^{c}=1,\pi^{d}=0\}$ if $s=C$ and $\pi_{ks} = \{\pi^{c}=0,\pi^{d}=1\}$ if $s=D$. If automaton $k$ is ALLD: $S_{k} \in \{D\}$, $s_{k0} = D$, $\phi_{k}(D,\omega) = D ~\forall~\omega\in \{c,d\}$, and $\pi_{ks} = \{\pi^{c}=0,\pi^{d}=1\}$ where $s = D$. 
\subsection{Matrix representation}
A set of strategies each represented as DFA $(S_{k}, s_{k0},\phi_{k}, \pi_{k})$ can be used as a candidate set by the estimation function. The candidate set is submitted to the estimation function in the form of an R matrix. To continue with the examples, the numbers embraced by the brackets in Figure \ref{fig:Matrix} define a candidate set which consists of the strategies TFT and ALLD.
\begin{figure}[htbp]
\caption{Matrix representation of a candidate set}
\label{fig:Matrix}
\begin{equation*}
\begin{matrix} 
~~\text{state}~ & ~~~\pi^{1}_{ks}~~~ & \phi(s,1) & \phi(s,2) & \phi(s,3) & \phi(s,4)\\
\end{matrix}
\end{equation*}\\
\vspace*{-1.0cm}
\begin{equation*}
\begin{bmatrix} 
~~~~~1~~~ & ~~~~1~~~~ &~~~~1~~~~ & ~~~~2~~~~ &~~~1~~~~ & ~~~~2~~~~~~  \\ 
~~~~~2~~~ & ~~~~0~~~~ &~~~~1~~~~ & ~~~~2~~~~ &~~~1~~~~ & ~~~~2~~~~~~  \\
~~~~~1~~~ & ~~~~0~~~~ &~~~~1~~~~ & ~~~~1~~~~ &~~~1~~~~ & ~~~~1~~~~~~   \\ 
\end{bmatrix} 
\end{equation*}
\end{figure}  

The first two rows of the matrix define the TFT automaton. The third line defines the ALLD automaton. Each row of the matrix corresponds to one state of a strategy, starting with the initial state $s_{k0}$ of an automaton. The labels on top of the matrix illustrate the information in the columns and are not part of the matrix which used in the estimation function. The first column enumerates the states of strategy $k$. Hence, the number one in the first column always indicates the beginning of a new automaton with its start state. Column two contains the element of the multinomial response vectors which predicts the lowest non-zero output value in the data. In the example, this is action $c$, since we will use the value one to indicate cooperation and zero for defection in the data. If there are more output values in the data, more columns have to be added after the second column which contain the response probabilities of the strategies for these output values. The response probability for outputs with the value zero, are always omitted. Hence, the matrix representation of TFT is sufficient if defection is indicated with the value zero in the data. If defection is instead indicated with the value 2, we would need to include another column after the second column with probability values that  row-wise sum up to one. 

Columns three to six define the deterministic transitions between states. The numbers in column four indicate the next state if the input is one. In the example, the input is one if the strategy profile of the previous period was $cc$, and the TFT automation moves on to state $C$. The numbers in column four indicate the next state if the input is two. In the example, the input is two if the action profile of the previous period was $cd$, and the TFT automation moves on to state $D$. The interpretation of columns five and six is the same. No column exists for the input 0 as this input always points to the start state unconditional of the current state. 

The system of rows and columns illustrated in the example can be used to define candidate sets for many other games. For example, imagine the goal is to analyze data of variant of the prisoner's dilemma with a third action $e$ labeled with the number two in the data. The matrix can be augmented by additional column which indicates the probability of response $e$ for every state. Action $e$ can also be used as an additional input which increases the number of possible inputs from five to ten (3 $\times$ 3 action profiles plus input 0). 

Generally speaking, the strategies matrix is a matrix where each row corresponds to one state of a strategy, starting with the start state $s_{k0}$ of an automaton. The first column enumerates the states of each strategy in ascending order. A value of one in the first column indicates the begin of a new strategy with its start state. The columns after the first column contain the collection of multinomial response vectors. The number of columns for the multinomial response vectors must correspond to the number of unique non-zero outputs in data. Without a reference output - which is labeled with a zero in the output column of data - the columns specify the complete multinomial response distribution for each unique value in the output column. In this case, the response probabilities in each row must sum to one. With a reference output, the response probability for the response labeled with zero is omitted and the response probabilities in each row must sum to a value smaller or equal to one. The remaining columns of the strategies matrix define the deterministic state transitions. The number of columns must equal the number of unique non-zero inputs in the data. 

\textsf{stratEst} contains a set of 22 strategies which have been used to describe behavior in the indefinitely repeated prisoner's dilemma \citep{DalBo2011,Fudenberg2012,Breitmoser2015}. A documentation of the strategies can be found in the \ref{sec:appendix}. The strategies documented in the \ref{sec:appendix} can also serve as further examples.
\section{Strategy Estimation}
\label{sec: Strategy estimation}
Strategy estimation was first used by \cite{DalBo2011} to estimate the population shares of a candidate set of strategies based on a sample of experimental data from the indefinitely repeated prisoner's dilemma. Since the original publication, strategy estimation has been employed in several other studies on the repeated prisoner's dilemma \citep[e.g.][]{Aoyagi2009,Aoyagi2017, Arechar2017,Camera2012,Embrey2013,Fudenberg2012,Breitmoser2015}. 

\cite{Breitmoser2015} extended the method by simultaneously estimating the relative frequency of strategies and some strategy-specific response parameters from experimental data. The possibility to estimate response parameters from data turns out to be useful when candidate strategies are ex-ante unknown or when some strategy parameters can't be pinned down based on theory \cite[as it is the case for the semi-grim strategies reported in][]{Breitmoser2015}. 

\cite{Dvorak2018b} extend strategy estimation in the spirit of latent class regression. In the latent class regression model, the prior probability to use a certain strategy is modeled as a function of individual characteristics which allows to asses the role of covariates for strategy use. 

Section \ref{sec: Strategy estimation} proceeds by introducing the general model, the algorithm and the parameter estimates for strategy estimation in Subsections \ref{subsec: Model definition} - \ref{subsec: Parameter estimation}. The latent class regression model receives separate treatment in Section \ref{sec:Latent class regression}. 

%Another approach which has been used to estimate the prevalence and structure of strategies is to ask participants of experiments to report their strategies as done in \cite{DalBo2017}, \cite{Embrey2016}, \cite{Mengel2011}, \cite{Romero2016}, and \cite{Selten1997}. Results for the repeated prisoner's dilemma indicate that the elicited strategies often correspond to some of the candidate strategies used in the maximum-likelihood estimations. Asking participants for their strategies is desirable but usually requires more time and programming effort \cite[see for example the well-designed strategy interface of][]{Romero2016}. 
%
%\cite{Camera2012} propose another method to estimate the prevalence of repeated game strategies. The method approximates the prevalence of a strategy by the number of times a strategy correctly predicts the behavioral response of a participant. The advantage of this method is that the estimated prevalence of a strategy is not sensitive to the composition of the candidate set of strategies. The disadvantage of the method is that it cannot be extended to estimate strategy parameters from the data, estimate the prevalence of behavior strategies, or select the number of strategies or strategy parameters based on information criteria. 

%The \textsf{stratEst} package coffers both variants of the strategy estimation method plus some . Additionally, the package includes several options to restrict theperform model selection based on one of several information criteria. Standard errors for the model parameters can be estimated analytically or based on a bootstrap procedure. In addition to these features, the package can be used to infer the response parameters of pure strategies from the data which is useful in situations where reasonable candidate strategies are not known. The package also features an extension of strategy estimation in the spirit of latent class regression which can be used to assess the role of covariates on strategy use.
\subsection{Model}
\label{subsec: Model definition}
This subsection presents the general model for strategy estimation. Consider a collection of categorical responses $r=\{1,\cdots,R\}$ of individuals $i = \{1,\cdots,N\}$ across  situations $j = \{1,\cdots,J\}$ of the same game. Each situation is characterized by a unique history of past play leading to situation $j$. The DFA representation of strategy $k$ assigns an internal state $s_{k} = \{1,\cdots,S_{k}\}$ to each situation $j$. The state determines the response of the individual in the current situation. The subscript $k$ which indicates that states are strategy-specific will be omitted for better readability. 

Let $y_{isr}$ denote the number of times response $r$ is observed in $n_{is}$ observations of individual $i$ when strategy $k$ would be in state $s$. The central assumption of the model is that the probability to observe vector $Y_{is} = \{y_{isr},\cdots,y_{isR}\}$ follows $n_{is}$ independent draws from a multinomial distribution with parameters $\pi_{ks} = \{\pi_{ks1},\cdots,\pi_{ksR}\}$ where $\pi_{ksr} \in [0,1]$ and $\sum_{r = 1}^{R} \pi_{ksr} = 1~\forall~s \in S_{k}$ and $k \in K$. The assumption implies that the behavior of individuals is exclusively determined by the strategy they use. As a result, responses should be conditionally independent when controlling for the underlying strategies. 

If $k$ is a pure strategy, the response probabilities $\pi_{ksr}$ are the result of pure underlying response probabilities confounded by trembling hand errors \citep{Selten1975}. Let $\xi_{ks}$ denote a vector of pure underlying response probabilities with elements $\xi_{ksr} \in \{0,1\}$ and $\gamma_{ks} \in [0,1]$ the probability of a tremble. The response probabilities $\pi_{ksr}$ follow from 
\begin{equation}
\label{eq: pi actual}
\pi_{ksr} = \xi_{ksr}(1-\gamma_{ks}) + (1-\xi_{ksr})\frac{\gamma_{ks}}{R-1}. 
\end{equation}
Equation \eqref{eq: pi actual} describes a process in which a tremble uniformly implements one of the other responses after a realization has been obtained based on the vector $\xi_{ks}$. The tremble rules out that a single response which is not predicted by a pure strategy results in a likelihood of zero that the individual uses the strategy.

\textsf{stratEst} estimates the maximum-likelihood shares $p_{k} \in [0,1]$ with $\sum_{k=1}^{K} p_{k} = 1$ of individuals in the population which follow strategy $k$ with strategy parameters $\pi_{ksr}$ or $\xi_{ksr}$ and $\gamma_{ks}$ in the case of pure strategies. If it was known which individual follows which strategy, $p_{k}$ would immediately follow and the maximum-likelihood estimates of the strategy parameters could be easily obtained. However, the assignments of individuals to strategies are unknown latent variables which have to be estimated from the data. In the incomplete model, the strategy shares $p_{k}$ indicate the prior probability that individual $i$ uses strategy $k$. The observed likelihood of the incomplete model is:
\begin{equation*}
L = \prod_{ i = 1}^{N} \sum_{ k = 1}^{K} p_{k}  \prod_{s = 1}^{S_{k}}  {n_{is}\choose y_{is1},\cdots,y_{isR}} \prod_{r=1}^{R}(\pi_{ksr})^{y_{isr}}
\end{equation*}
Since the multinomial coefficients are constant factors of the likelihood function, \textsf{stratEst} maximizes the log-likelihood function
\begin{equation}
\label{eq: ln L}
\text{ln}L = \sum_{ i = 1}^{N} \text{ln} \left( \sum_{ k = 1}^{K} p_{k} \prod_{s  = 1}^{S_{k}} \prod_{r=1}^{R} (\pi_{ksr})^{y_{isr}} \right).
\end{equation}
\textsf{stratEst} reports the parameters $p_{k}^{*}$, $\pi_{ksr}^{*}$ which maximize \eqref{eq: ln L} under the parameter constraints $\pi_{ksr} \in [0,1]$ and $\sum_{r = 1}^{R} \pi_{ksr} = 1$, and $p_{k} \in [0,1]$ and $\sum_{k=1}^{K} p_{k} = 1$. 
\subsection{Algorithm}
\label{subsec: Algorithm}
\textsf{stratEst} uses the Expectation-Maximization algorithm \cite[EM,][]{Dempster1977} to obtain the maximum-likelihood estimates $p_{k}^{*}$, $\pi_{ksr}^{*}$ of the incomplete data problem outlined in \eqref{eq: ln L}. The algorithm exploits the fact that the ML estimates of the strategy parameters can be inferred if the assignments of individuals to strategies are known. At the same time, for known strategy parameters, the computation of the posterior probability estimates of the assignments of individuals to strategies is straightforward. After constrained random initialization of the model parameters, the EM algorithm iterates between the two steps until convergence of the log-likelihood defined in \eqref{eq: ln L}. In the expectation step, the posterior probability that individual $i$ uses strategy $k$ is calculated based on the current values of the parameters $p_{k}$ and $\pi_{ksr}$ according to:
\begin{equation}
\label{eq: theta new}
\theta_{ik} = \frac{ p_{k} \prod_{s = 1}^{S_{k}} \prod_{r=1}^{R} (\pi_{ksr})^{y_{isr}}}{\sum_{ k = 1}^{K} p_{k} \prod_{s = 1}^{S_{k}} \prod_{r=1}^{R} (\pi_{ksr})^{y_{isr}} }.
\end{equation}
In the maximization step, the posterior probability assignments are used to update the population shares and strategy parameters. The population shares $p_{k}$ are updated to the expected values of the posterior probability assignments. The strategy parameters $\pi_{ks}$ are updated based on $K$ weighted data sets. To obtain the weighted data for strategy $k$, the responses $Y_{is}$ of individual $i$ are considered proportional to the posterior probability that $i$ uses $k$. The two steps are subsequently repeated until the log-likelihood converges. \smallskip\\
Depending on the starting values used, the EM algorithm may converge to local optima. To avoid that local optima are returned by the estimation function, \textsf{stratEst} executes several runs of the EM algorithm from different starting points. \textsf{stratEst} uses the procedure proposed by \cite{Biernacki2003} to avoid local optima in an efficient way. During an outer run of the solver, several short inner runs of the EM algorithm are performed from different starting points and only the best solution obtained from the short runs is followed until convergence. \textsf{stratEst} reports the best solution found in several outer runs of the solver. 
\subsection{Parameter estimation}
\label{subsec: Parameter estimation}
To find the maximum-likelihood estimates $p^{*}_{k}$, $\pi^{*}_{ksr}$, the \textsf{stratEst} solver starts by randomly initializing parameter values participant to the parameter constraints. In the expectation step of each iteration, the posterior probability that individual $i$ uses strategy $k$ is updated based on the current values of the model parameters according to \eqref{eq: theta new}. In the maximization step of each iteration, the model parameters are updated in order to maximize \eqref{eq: ln L} conditional on the updated posterior probability assignments. For the strategy shares $p_{k}$, this requires optimization with respect to a sum-to-one constraint which is achieved based on the Lagrange multiplier function
\begin{equation*}
\Lambda (p_{k},\lambda) = \text{ln} L  + \lambda \left( \sum_{k = 1}^{K} p_{k} - 1 \right). 
\end{equation*}
Setting the partial derivatives $\partial \Lambda / \partial p_{k}$ and $\partial \Lambda / \partial \lambda $ to zero and solving for $p_{k}$ and $\lambda$ yields the conditions
\begin{equation*}
p_{k} = - \sum_{i=1}^{N} \frac{ \theta_{ik} } {\lambda } \quad\text{and}\quad  \sum_{k = 1}^{K} p_{k} = 1 
\end{equation*}
which together yield $\lambda = -N$. Substitution into the first condition shows that the updated values of the shares follow from the updated posterior probability assignments calculated in the expectation step since
\begin{equation}
\label{eq: p new}
p_{k}^{new} =  \frac{\sum_{i=1}^{N} \theta_{ik} } { N }.
\end{equation}
If some shares $p_{k^{\prime}}$ with $k^{\prime} \in K$ have fixed values specified by the user, the remaining shares are updated according to \eqref{eq: p new} and subsequently rescaled by $1-\sum_{k^{\prime} \in K} p_{k^{\prime}}$ to fulfill the sum-to-one constraint. The response probabilities $\pi_{ksr}$ are also updated participant to the sum-to-one constraint. Using Lagrange multipliers the updated values follow from
\begin{equation}
\label{eq: pi new}
\pi_{ksr^{new}} = \sum_{ i = 1}^{N} \frac{\theta_{ik} y_{isr}}{ \sum_{ i = 1}^{N} \theta_{ik} n_{is}  }.
\end{equation}
Again, if parameters $\pi_{ksr\prime}$ with $r^{\prime} \in R$ are fixed, the remaining updated parameters are rescaled by $1-\sum_{r^{\prime} \in R}  \pi_{ksr^{\prime}}$ to fulfill the sum-to-one constraint.

\textsf{stratEst} uses the following procedure to update the pure underlying response parameters and the corresponding trembles. The pure response parameters are updated by transforming the updated values of the corresponding mixed parameters $\pi_{ksr}$ according to
\begin{equation}
\label{eq: pure xi new}
\xi_{ksr}^{new} =  \begin{cases} 1 &  \text{if}~ \pi_{ksr}^{new} > \pi_{ksr^{\prime}}^{new}~\forall~ r^{\prime} \neq r \\
0 & \text{otherwise}. \end{cases}
\end{equation} 
Equation \ref{eq: pure xi new} assigns density of one to the maximum of the updated response vector $\pi_{ks}^{new}$. This assures that the corresponding tremble parameters $\gamma_{ks}$ are as small as possible. If the maximum is not unique, the first parameter is set to one and all others values to zero. The updated values of the trembles can be found based on the substitution of \eqref{eq: pi actual} into \eqref{eq: pi new}. For the update of the tremble all response probabilities affected by the tremble are taken into account which yields
\begin{equation}
\label{eq: gamma new}
\gamma_{ks}^{new} =  \sum_{ i = 1}^{N} \frac{  \theta_{ik} \sum_{r=1}^{R} ( y_{isr} - n_{is} \xi_{ksr}^{new} ) \left( \frac{R-1}{1- R \cdot \xi_{ksr}^{new}} \right)}{ \sum_{ i = 1}^{N} \theta_{ik}  \cdot R \cdot n_{is} }.
\end{equation}

Whenever parameters specified by the user are pure (i.e. zero or one), \textsf{stratEst} will automatically estimate a tremble parameter for these parameters. For mixed parameters, no tremble is estimated. Generally, for all response parameters which are estimated from the data, the restriction applies that all estimated parameters have to be of the same type either pure or mixed. Strategy parameters specified by the user are not affected by this restriction and can be pure or mixed independent of the type of the estimated parameters.

After all model parameters have been updated based on \eqref{eq: p new} and \eqref{eq: pi new}, the log-likelihood of the updated model is determined based on \eqref{eq: ln L} and compared to the value from the last iteration. The algorithm continues with the next iteration as long as the log-likelihood increased in the current iteration. 
\section{Model Selection}
\label{sec:Model selection}
The number of free parameters of a completely unspecified model with mixed responses equals $(K-1) + (R-1) \cdot \sum_{k=1}^{K} S_{k}$. Depending on the number of strategies, states and responses the number of free model parameters can be quite large. Four different approaches can be used to reduce the number of model parameters. 
\begin{enumerate}
\item The conventional approach fixes parameters to specific values based on theoretical considerations. Fixed parameters are not estimated and reduce the number of free model parameters on a one to one basis. 
\item Restrictions can be imposed on the strategy parameters. The restrictions imply that all model parameters of the same family ($\pi$, $\gamma$) which are affected by the restriction are reduced to a single vector of parameters. Three variants exist. Either each parameter of the same family is replaced by a single parameter vector for each strategy, for each state or overall. 
\item  The number of parameters of the same family can be selected based on information criteria. Three variants exist. The optimal number of parameter vectors of the same family is selected for each strategy, for each state or overall. 
\item The number of strategies used to describe the data can be selected based on information criteria. For a candidate set strategies, nested models with fewer strategies are estimated and the best model is selected. 
\end{enumerate}
The first approach is achieved by fixing the elements of inputs objects at specific values and illustrated in Section \ref{sec:Using stratEst}. The second, third , and fourth approaches are discussed in the following Subsections \ref{subsec:Restrictions on strategy parameters}, \ref{subsec:Selection of the number of strategy parameters}, and \ref{subsec:Selection of the number of strategies}.   
\subsection{Restrictions on strategy parameters}
\label{subsec:Restrictions on strategy parameters}
By default strategy parameters are assumed to be strategy-state specific, i.e. a response vector $\pi_{ks}$ is estimated for every state of every strategy in the case of mixed responses and a response vector $\xi_{ks}$ plus a tremble $\gamma_{ks}$ is estimated in the case of pure responses. One possibility to reduce the number of free parameters is to impose restrictions that some of the estimated strategy parameters from the same family are reduced to a single parameter vector. Restrictions can be imposed independently on the parameter vectors $\pi_{ks}$ and the trembles $\gamma_{ks}$. In both families of parameters, \textsf{stratEst} offers three variants of restrictions. Parameters can either be strategy-specific, state-specific or global. The first variant estimates a single parameter vector $\pi_{k}$ or a single parameter vector $\xi_{k}$ and a single tremble $\gamma_{k}$ per strategy which applies in all states of the strategy. The second variant estimates a single parameter vector $\pi_{s}$ or a single parameter vector $\xi_{s}$ and a single tremble $\gamma_{s}$ which applies in all states with the same number in the first column of the strategy matrix which enumerates the states. The third variant estimates a single parameter vector $\pi$ or a single parameter vector $\xi$ and a single tremble $\gamma$ which apply globally across all states and strategies. It is up to the user to decide if any of the restrictions can be justified based on theory. Please note that the second variant does not take into account whether states have the same deterministic state transitions across strategies. \smallskip\\
If restrictions to the strategy parameters apply, the maximization step in the parameter estimation is adapted accordingly. Let $Z_{t}$ denote the set of all states $s_{k}$ of strategy $k$ where the corresponding strategy parameters are restricted to have the same underlying parameter vector $\zeta_{t}$, where $t \in \{1,\cdots,T\}$ is an index for the restrictions. The individual score contributions to $\zeta_{t}$ take all parameters affected by restriction $t$ into account, i.e.
\begin{equation}
\label{eq: pi new restricted}
\pi_{tr}^{new} = \sum_{ i = 1}^{N} \sum_{k=1}^{K} \sum_{ s \in Z_{t} }\frac{  \theta_{ik} y_{isr}}{ \sum_{ i = 1}^{N}    \sum_{ s \in Z_{t} } \theta_{ik} n_{is_{k}}  }
\end{equation}
if $\zeta_{t}$ is a vector of response parameters. If the responses are pure, the updated response parameters are processed as described in Subsection \label{subsec: Pure strategy parameters} and the trembles $\zeta_{t}$ are updated according to
\begin{equation}
\label{eq: gamma new restricted}
\gamma_{t}^{new} =  \sum_{ i = 1}^{N} \sum_{k=1}^{K}  \sum_{ s \in Z_{t} }\frac{ \theta_{ik} \sum_{r=1}^{R} ( y_{isr} - n_{is} \xi_{ksr}^{new} ) \left( \frac{R-1}{1- R \cdot \xi_{ksr}^{new}} \right) }{ \sum_{ i = 1}^{N}   \sum_{ s \in Z_{t} } \theta_{ik}  \cdot R \cdot n_{is}} .
\end{equation}


\subsection{Selection of the number of strategy parameters}
\label{subsec:Selection of the number of strategy parameters}
\textsf{stratEst} executes the following procedure to select the number of strategy parameters $\pi$, $\xi$ and $\gamma$ based on one of the three information criteria outlined in Subsection \ref{subsec:Information criteria}. First, an unrestricted model is estimated with a different parameter vectors for every strategy-state combination. A strategy-state-specific response vector $\pi_{ks}$ is estimated if the response is mixed, and a strategy-state-specific response vector $\xi_{ks}$ and a tremble $\gamma_{ks}$ are estimated if the response is pure. Next, a set of candidate parameter vectors of the same family is identified. The candidate sets of parameter vectors which are participant the selection can be restricted based on the three variants. The selection procedure for strategy parameters can be restricted to select the parameter vectors globally, for states across all strategies, or within strategies. For every pairwise combination of the elements of the set of candidate parameter vectors, a model is estimated where two candidate parameter vectors are reduced to a single parameter vector. The procedure continues as long as the fusion of any combination of two candidate parameter vectors improves the information criterion specified by the user. 

%\subsection{Selection of the number of states}
%\label{subsec:Selection of the number of states}
%stratEst executes the following procedure to select the number of states within the strategies for which strategy parameters are estimated. In the finite-state representation states are equivalent if in states $q$ and $c$ of strategy $k$ the finite-state transitions are identical and $\pi_{q} = \pi_{c}$ holds. If the estimated strategy parameters are mixed, it is sufficient that the true underlying response probabilities of both states are the same since trembles are zero. If the estimated strategy parameters are pure, the condition above requires that the true underlying response probabilities and the trembles take the same value. Since the finite-state transitions are fixed and set by the user, these inputs control which sates will be checked for equivalence. \smallskip\\
%The procedure to select the number of states departs from the maximum-likelihood estimates of the model parameters which are obtained under the assumption that all states with free parameters are distinct. For any combination $r \in \{1,\cdots,\sum_{k=1}^{K} {S_{k} \choose 2} \} $ of two distinct states $q$ and $c$ within one strategy the log-likelihood of a restricted model is estimated where the number of parameters is reduced by one compared to the unrestricted model. The only difference between the candidate model and the unrestricted model is that the two response probabilities of the states $q$ and $c$ are replaced by a single parameter $\omega_{r}$. To obtain a value for $\omega_{r}$ we use one maximization step based on \eqref{eq: restriction} where the parameters of all other states are the maximum-likelihood estimates of the unrestricted model. The EM algorithm is executed based on the parameters of the restricted model $r$ which produces the highest log-likelihood based on \eqref{eq: ln L}. The procedure is repeated as long as the result obtained after convergences improves the value of the selection criterion specified by the user. 
\subsection{Selection of the number of strategies}
\label{subsec:Selection of the number of strategies}
The number of strategies can be reduced based on a procedure introduced by \cite{Breitmoser2015}. The procedure starts by estimating the complete model with $K$ strategies. Next, $K$ nested models which result from deleting one strategy from the set of $K$ strategies are estimated. The $K$ nested models are ranked according to their log-likelihood. The procedure starts with the nested model with the lowest log-likelihood and compares the value of the information criterion with the value of the complete model with $K$ strategies. If eliminating the strategy is recommended based on the information criterion, the procedure is repeated starting with the reduced model with $K-1$ strategies. Otherwise, the nested model with the second lowest log-likelihood is considered. The strategy selection procedure stops if no strategy can be eliminated.
\subsection{Information criteria}
\label{subsec:Information criteria}
Three different penalized-likelihood criteria can be used to select the number of strategy parameters and strategies. The criteria are the Akaike Information Criterion \citep[AIC,][]{Akaike1973}, the Bayesian Information Criterion \citep[BIC,][]{Schwarz1978}, and the Integrated Classification Likelihood \citep[ICL,][]{Biernacki2000}. The formulas for the three model selection criteria are
\begin{equation*}
\text{AIC} = -\text{ln}L + df 
\end{equation*}
\begin{equation*}
\text{BIC} = -\text{ln}L + \frac{df}{2}log( N )  
\end{equation*}
\begin{equation*}
\text{ICL} = \text{BIC} - \sum_{i = 1}^{N} \sum_{K = 1}^{K} \theta_{ik} log(\theta_{ik})  ,
\end{equation*}
In all three formulas, $df$ represents the number of free parameters of the model. Different assumptions and asymptotic approximations are needed to derive the formulas above. From the practitioner's point of view, the main difference between the three criteria is the size of the penalty for model complexity. ICL penalizes complexity more than BIC, and BIC more than AIC. In practice, all three model selection criteria will often deliver the same results. It is recommended to use AIC when it is more important to avoid underfitting of the data, and BIC when it is more important to avoid overfitting of the data. As $ICL$ includes an extra penalty for the entropy of the posterior probability assignments, it is recommended to use $ICL$ whenever precise predictions of individual strategy use are important.  
\section{Latent Class Regression}
\label{sec:Latent class regression}
\textsf{stratEst} features latent class regression to assess the role of covariates for strategy use. Using the posterior probability assignments of individuals to strategies as dependent variables in a multinomial model leads to downward biased coefficients for the effects of covariates \citep{Bolck2004}. Latent class regression \citep{Dayton1988,Bandeen1997} is a method which models the prior probability that individual $i$ uses strategy $k$ as a function of covariates. The simultaneous estimation of model parameters and coefficients generates unbiased estimates for the effect of the covariates.
\subsection{The multinomial latent class model}
\label{subsec:Multinomial latent class model}
\textsf{stratEst} uses the generalized multinomial logit link function to model the effects of covariates on the prior probability that individual $i$ uses strategy $k$ \citep[see][for an introduction]{Agresti2003}. The model takes the probability to use the first strategy as the benchmark. Then the the log-odds of using strategy $k$ compared to the first strategy are modeled as a linear function of covariates. \textsf{stratEst} will automatically add an intercept which will result in a covariate matrix $X$ with $N$ rows and $C$ columns if $C-1$ variables are supplied. Let $X_{i}$ denote the the $i^{th}$ row of the covariate matrix $X$, then:
\begin{equation*}
\text{ln}(p_{ik}/p_{i1}) = X_{i}\beta_{k} ~\forall~k \in K
\end{equation*}
where $p_{ik}$ represent the prior probability that individual $i$ uses strategy $k$ and $\beta_{k}$ is a column vector of coefficients with $C$ elements. Algebraic manipulations of the $K$ equations above yield
\begin{equation}
p_{ik} = \frac{e^{X_{i}\beta_{k}}}{\sum_{k=1}^{K} e^{X_{i}\beta_{k}}}
\end{equation}
and the posterior probability that individual $i$ uses strategy $k$ is now
\begin{equation}
\label{eq: latent theta new}
\theta_{ik} = \frac{ p_{ik} \prod_{s = 1}^{S_{k}} \prod_{r=1}^{R} (\pi_{ksr})^{y_{isr}}}{\sum_{ k = 1}^{K} p_{ik} \prod_{s = 1}^{S_{k}} \prod_{r=1}^{R} (\pi_{ksr})^{y_{isr}} }.
\end{equation}
The log-likelihood function of the latent class regression model is:
\begin{equation}
\label{eq: ln L latent class regression}
\text{ln}L = \sum_{ i = 1}^{N} \text{ln} \left( \sum_{ k = 1}^{K} p_{ik} \prod_{s  = 1}^{S_{k}} \prod_{r=1}^{R} (\pi_{ksr})^{y_{isr}} \right).
\end{equation}

\subsection{Parameter estimation}
\label{subsec:Parameter estimation}
To goal of latent class regression is to identify the maximum-likelihood parameters $\beta^{*}$ and $\pi_{ksr}^{*}$ which maximize \eqref{eq: ln L latent class regression}. This is achieved based a variant of the EM algorithm which uses a Newton-Raphson step to update the coefficients during the maximization step of the EM algorithm \citep{Bandeen1997}. After initialization, the expectation step consists of calculating the posterior probabilities $\theta_{ik}$ according to \eqref{eq: latent theta new}. In the maximization step, the column vector of coefficient $\beta$ is updated using
\begin{equation}
\label{eq: new beta}
\beta^{new} = \beta - H_{\beta}^{-1} \triangledown_{\beta}
\end{equation}
where $\triangledown_{\beta}$ is the score of the coefficient vector with elements:
\begin{equation}
\label{eq: score beta}
\frac{\partial \text{ln}L}{\partial \beta_{qk}  } = x_{iq}( \theta_{ik} - p_{ik})
\end{equation}
and $H_{\beta}$ is the Hessian of \eqref{eq: ln L latent class regression} for the coefficients with elements
\begin{equation}
\label{eq: hessian beta}
\frac{\partial^{2} \text{ln}L}{\partial \beta_{bl} \partial  \beta_{ck} } = \sum_{i=1}^{N} x_{ib}x_{ic}(\theta_{il}(\delta_{lk} - \theta_{ik}) - p_{il}(\delta_{lk} - p_{ik}))
\end{equation}
where $l,k \in \{1,\cdots,K\}$ and $b,c \in \{1,\cdots,C\}$ and $\delta_{lk} = 1$ if $l = k$ and $\delta_{lk} = 0$ otherwise. Note that in order to calculate $p_{ik}$, for individual $i$, the row vector $X_{i}$ must be complete and missing values are not allowed in the covariate matrix.
%\subsection{Maximum a posteriori estimation}
%\label{subsec:Maximum a posteriori estimation}
%In many instances when latent class regression is performed, the estimated model will suffer from quasi-complete separation. Quasi-complete separation arises if strategy choice is nearly perfectly predicted by the covariates for some individuals. Unfortunately, this will often be the case with the samples from experiments. Even though perfect prediction of strategy choice underlines the goodness of fit of the estimated model, it is problematic for the estimated standard errors of the latent class regression coefficients obtained via maximum likelihood (cite). The regression coefficients might not have a maximum-likelihood estimate at all and can take high values. Standard errors will be meaningless in this case. To avoid the problem, the individual priors which result from the latent class regression should always be checked for values close to zero or one. If such values can be found, the corresponding regression coefficients can take high values and do probably not represent the maximum likelihood estimate of these parameters. To correct for quasi-complete separation, a maximum a posteriori estimation can be performed assuming normal priors with mean zero for the regression coefficients. The posterior likelihood of the model is proportional to:
%\begin{align*}
%\text{ln} L^{p} = \text{ln} L - \sum_{b=1}^{B}\frac{\beta^{2}}{2\sigma^{2}}
%\end{align*}
%where $B$ is the number of latent class regression coefficients with $B = C(K-1)$. The factor $1/\sigma^{2}$ can be interpreted as a penalty term for large values of regression coefficients which can be set by the user. The penalty shifts larger coefficients towards zero and produces finite estimates with reasonable standard errors under quasi-complete separation. Note that these shifts do not affect the interpretation of the effects much: If the value of a coefficient is $100$, this indicates an odds-ratio of $2.6e+43$, while the odds ratio is $2.2e+4$ for a coefficient of $10$. However, the difference in the odds ratios will not be important with normal sample sizes. Maximum a posteriori estimation of Latent Class Coefficients with normally distributed priors is analyzed based on simulations in (Vermunt et al). The results show that the MAP estimates and corresponding standard errors are in most cases superior to their ML counterparts.
\section{Standard Errors}
\label{sec:Standard errors}
By default analytic standard errors are reported for all estimated model parameters. The estimation strategy rests on the assumption that the individuals' strategy use is the result of independent realizations of the same stochastic process. This assumption might be violated for instance due to matching group or session effects in experiments. Unfortunately, using cluster robust standard errors is not a solution to this problem. Parameter estimates will be biased in cases where normal and cluster robust standard errors diverge due to the non-linearity of the model \citep[see][]{King2015}. One way to deal with such data is to use the same individual identifier for all observations which belong to the same cluster. The reported posterior probability assignments can then be interpreted as the shares of the strategies in each cluster. For the following calculations is assumed that the strategy use of the entities of interest $i \in \{1,\cdots,N\}$ follow independent draws from the prior distribution of strategies. 

\subsection{Analytic standard errors}
\label{subsec:Analytic standard errors}
\textsf{stratEst} estimates analytic standard errors based on the \textit{empirical observed} information matrix \citep{Meilijson1989} \cite[see][for an earlier application of the estimation procedure]{Linzer2011}. The \textit{empirical observed} information matrix is defined as
\begin{equation}
\label{eq:Ie}
I_{e}(Y,\hat{\Psi}) = \sum_{i=1}^{N} s(Y_{i},\hat{\Psi})s^{T}(Y_{i},\hat{\Psi}),
\end{equation}
where $s(Y_{i},\hat{\Psi})$ is the score contribution of individual $i$ with respect to parameter vector $\Psi$, evaluated at the ML estimate $\hat{\Psi}$. The reported standard errors are the square roots of the main diagonal of the inverse of $I_{e}(Y,\hat{\Psi})$. To calculate the standard error of the parameter $\eta_{r}$ with $\sum_{r=1}^{R} \eta_{r} = 1 $, the score function $s(Y_{i},\hat{\eta_{r}})$ is reparameterized in terms of log-ratios $\mu_{r} = ln(\eta_{r}/\eta_{1})$ and the variance-covariance matrix VAR$(\eta)$ is calculated based on \eqref{eq:Ie}. The variance-covariance matrix VAR$(\eta)$ of the parameters is approximated using the delta method by
\begin{equation}
\label{eq:Delta}
\text{VAR}(f(\hat{\mu})) = f^{\prime}(\mu) I_{e}(Y,\hat{\mu})^{-1} f^{\prime}(\mu)^{T},
\end{equation}
where $f^{\prime}(\mu)$ is the Jacobian of the function $f(\mu_{r}) = \eta_{r} = e^{mu_{r}}/\sum_{r=1}^{R} \mu_{r}$ which converts the values back to the original units.
 
\textsf{stratEst} uses the following score vectors to calculate the empirical observed information matrix defined in \eqref{eq:Ie}. The shares are reparameterized in terms of log-rations as $p^{*}_{k} = ln(p_{k}/p_{1})$ and the score contribution $\partial \text{ln}L / \partial p^{*}_{k}$ of individual $i$ is
\begin{equation}
\label{eq:score p}
s(Y_{i},p^{*}_{k}) =  \theta_{ik}  - p_{k}. 
\end{equation}
Let $f(p^{*}_{k}) = p_{k} = e^{p^{*}_{k}}/\sum_{l=1}^{K} e^{p^{*}_{l}}$ denote the inverse of the reparameterization, then the Jacobian $f^{\prime}(p^{*})$ has elements 
\begin{equation}
\label{eq:jacob p}
\frac{\partial f(p^{*}_{k}) }{\partial p^{*}_{l}} =  \begin{cases} -p_{k} p_{l}  &  \text{if}~ l \neq k \\
p_{k}(1-p_{l}) & \text{if}~ l = k \end{cases}
\end{equation}
and the variance-covariance matrix of the shares is approximated by \eqref{eq:Delta} using the inverse of \eqref{eq:Ie} based on the score contributions defined in \eqref{eq:score p}. 

If $\pi_{ksr}$ are mixed parameters standard errors are calculated based on the reparameterization $\pi^{*}_{ksr} = ln(\pi_{ksr}/ \pi_{ks1})$ and the score contribution $\partial \text{ln}L / \partial \pi^{*}_{ksr}$ of individual $i$ is
\begin{equation}
\label{eq:score pi}
s(Y_{i},\pi^{*}_{ksr}) =  \theta_{ik} \left( y_{isr} - n_{is}\pi_{ksr} \right).
\end{equation}
Let $g(\pi^{*}_{ksr}) = \pi_{ksr} = e^{\pi^{*}_{ksr}}/\sum_{r=1}^{R} \pi^{*}_{ksr}$ denote the inverse of the reparameterization, then the Jacobian $g^{\prime}(\pi^{*})$ has elements 
\begin{equation}
\label{eq:jacob pi}
\frac{\partial g(\pi^{*}_{ksr}) }{\partial \pi^{*}_{ltq}} =  \begin{cases} - \pi_{ksr} \pi_{ltq}  &  \text{if}~k = l~\text{and}~s = t~\text{and}~r \neq q \\
\pi_{ksr}(1-\pi_{ltq}) & \text{if}~k = l~\text{and}~s = t~\text{and}~r = q \\
0 & otherwise \end{cases}
\end{equation}
and the variance-covariance matrix of the shares is approximated by \eqref{eq:Delta} using the inverse of \eqref{eq:Ie} based on the score contributions defined in \eqref{eq:score pi}.

No reparameterization is needed to obtain the standard errors of the tremble parameters since for any value of the tremble $\gamma_{ks}$ the sum-to-one constraint is always fulfilled for all affected response probabilities. The score contribution $\partial \text{ln}L / \partial \gamma_{ks}$ of individual $i$ is
\begin{equation}
\label{eq:score tremble}
s(Y_{i},\gamma^{*}_{ks}) =  \theta_{ik} \sum_{r=1}^{R} \frac{y_{isr}}{\pi_{ksr}} \left(  \frac{1-\xi_{ksr}}{R - 1} -\xi_{ksr} \right)
\end{equation}
and the variance-covariance matrix is approximated by the inverse of \eqref{eq:Ie} using the score of the coefficients outlined in \eqref{eq:score tremble}.

The reported standard errors of the latent class regression coefficients $\beta_{ck}$ are the square roots of the main diagonal of \eqref{eq:Ie} using the score of the coefficients outlined in \eqref{eq: score beta}.

\subsection{Analytic standard errors with parameter restrictions}
\label{subsec:Standard errors with parameter restrictions}
If restrictions apply, the score vectors change as before where the score contribution of individual $i$ is the summation over all states $s_{k} \in Z_{t}$ where parameters are affected by restriction $t \in \{1,\cdots,T\}$. For mixed response probabilities, the score contribution with respect to the underlying common parameter $\partial \text{ln}L / \partial \pi^{*}_{tr}$ of individual $i$ is
\begin{equation}
\label{eq:restricted score pi}
s(Y_{i},\pi^{*}_{tr}) =  \sum_{k=1}^{K} \theta_{ik} \sum_{s_{k} \in Z_{t}}  \left( y_{isr} - n_{is}\pi_{ksr} \right).
\end{equation}
and the Jacobian $g^{\prime}(\pi^{*})$ has elements 
\begin{equation}
\label{eq:restricted jacob pi}
\frac{\partial g(\pi^{*}_{tr}) }{\partial \pi^{*}_{uq}} =  \begin{cases} - \pi_{tr} \pi_{uq}  &  \text{if}~t = u~\text{and}~r \neq q \\
\pi_{t}(1-\pi_{u})   &  \text{if}~t = u~\text{and}~r = q \\
0 & otherwise. \end{cases}
\end{equation}

With restrictions, the score contribution of a tremble parameter $\partial \text{ln}L / \partial \gamma_{t}$ of individual $i$ is
\begin{equation}
\label{eq:restricted score tremble}
s(Y_{i},\gamma_{t}) =  \sum_{k=1}^{K} \sum_{s_{k} \in Z_{t}} \theta_{ik} \sum_{r=1}^{R} \frac{y_{isr}}{\pi_{ksr}} \left(  \frac{1-\xi_{ksr}}{R - 1} -\xi_{ksr} \right)
\end{equation}
All other model parameters are not affected by the restrictions and calculated as outlined before.

\subsection{Bootstrapped standard errors}
\label{subsec:Bootstrapped standard errors}
\textsf{stratEst} obtains bootstrapped standard errors for the population shares and strategy parameters by resampling individuals with replacement. In each bootstrap sample $m\in M$, parameter estimates are obtained based on the observations of $N$ individuals sampled in iteration $m$. Estimates for the strategy parameters are generated by fixing the value of all remaining strategy parameters of the model at the original maximum-likelihood estimate for these parameters. Fixation of the other model parameters is crucial to maintain the original structure of the model across the bootstrap estimates.  

When performing latent class regression, analytic standard errors are reported for the average priors $p_{k}$ and the coefficient vector $\beta$. The reason is that it is likely to produce samples which suffer from quasi-complete separation during the bootstrap procedure. Under quasi-complete separation, ML-estimate for the latent class regression coefficients may not exist in some bootstrap replications. In these replications, estimates of the latent class regression coefficients take very large values and bias the bootstrapped standard errors for these estimates. 


\section{Simulation}
\label{sec:simulation}
This section illustrates the validity of the estimation procedures based on simulated data. $M = 1000$ samples are generated. Each sample $m \in \{1,\cdots,M\}$ consists of the binary responses of $N=200$ individuals following one of the following two strategies:
\begin{equation*}
s_{1} = \begin{bmatrix} 1 & 1 - \gamma_{m} & 1 & 2   \\ 2 & \gamma_{m} & 1 & 2  \end{bmatrix};~s_{2} = 
\begin{bmatrix} 1 & \pi_{m} & 1 & 2   \\ 2 & \pi_{m} & 1 & 2  \end{bmatrix}
\end{equation*}
The parameters $\pi_{m}$ and $\gamma_{m}$ are independent draws from a normal distribution with mean $0.25$ and variance $0.1$.  The assignment of individuals to strategies in sample $m$ is influenced by covariate vector $x_{i}$ which contains an intercept and a dummy variable for individual $i$. The dummy variable is one for half of the individuals and zero for the other half. The probability of using $s_{i}$ for individual $i$ is given by:
\begin{align*}
Pr(s = s_{1}|x_{i}) = \frac{1}{1 + e^{x_{i} \beta_{m} }}
\end{align*}
The elements of the coefficient vector $\beta_{m}$ are the intercept $\beta_{0m}$ and the coefficient of the dummy variable $\beta_{1m}$. For each sample $m$, the coefficients $\beta_{0m}$ and $\beta_{1m}$ are independent draws from a normal distribution with mean zero and variance of one. Individuals are randomly assigned to $s_{1}$ with $Pr(s_{i} = s_{1}|x_{i})$. 
In every sample $m$, $20$ binary responses are generated for each individual in each of the two states. The binary responses of an individual are independent realizations of a Bernoulli process with success probability equal to the state specific response probability of the strategy the individual has been assigned.

The following matrix is used to submit the strategies to the solver:
\begin{equation}
\text{\texttt{strategies = } }\begin{bmatrix} 1 & 1 & 1 & 2  \\ 2 & 0 & 1 & 2 \\ 1 & \text{NA} & 1 & 2  \\ 2 & \text{NA} & 1 & 2  \end{bmatrix} 
\end{equation}
The first two rows of the matrix correspond to $s_{1}$ and the second and third row to $s_{2}$. \textsf{stratEst} will automatically estimate tremble parameters for both states of the first strategy if the submitted response probabilities are one and zero. Setting the response parameters of the second strategy to NA implies that these parameters should be estimated from the data. To estimate the correct model specification, the estimation is restricted to only one response parameter $\pi$ and one tremble parameter $\gamma$ probability. To include covariates in the estimation, a matrix $covar$ is generated with one column and as many rows as data. Each row of $covar$ which contains an observation of individual $i$ is set to the dummy variable of individual $i$. For each sample $m$, the following syntax is used to estimate the correct model:\bigskip\\
\texttt{stratEst(data,strategies,covar,r.responses ="global",r.trembles ="global")}\bigskip\\
Omitting the restrictions \texttt{r.responses ="global"} and \texttt{r.trembles ="global"} will estimate two response parameters - one for each occurrence of \texttt{NA} in strategies - and two tremble parameters - one for each occurrence of one or zero in strategies - respectively.
\begin{table}[htbp]
	\caption{Simulation exercise with two strategies}
	\label{tab:simulation stratEst}
	\centering 	
\begin{small}
		\begin{tabular}{p{15cm}}
			\hline\hline \\     [-2.5ex]
		\end{tabular}
		\begin{tabular}{lcccccc}
		 & & & & \multicolumn{2}{c}{coverage probability} & selection\\ [0.5ex]
		 \cline{5-6} \\[-2.5ex]
 $\theta_{m}~~~~$ & $~~~E(\hat{\theta}_{m})~~~$ & $~E(\theta_{m} - \hat{\theta}_{m})~$ & $E(|\theta_{m} - \hat{\theta}_{m}|)$ & $~~~$analytic$~~~$ & $~~~$bootstrap$~~~$& correct\\ 
\midrule
$p_{1}$ & 0.498 &  0.000 & 0.026 & 0.957 & 0.956 & 0.999\\
$p_{2}$ & 0.502 &  -0.000\textcolor{white}{-} & 0.026 & 0.957 & 0.956 & - \\
$\pi$ & 0.250 &  0.000 & 0.006  & 0.938 & 0.937 & 0.198\\
$\gamma$ & 0.252 &  -0.000\textcolor{white}{-} & 0.006 & 0.946 & 0.944 & 0.384\\
$\beta_{0}$ & -0.023\textcolor{white}{-} & 0.007 & 0.194 & 0.956 & 0.959 & -  \\
$\beta_{1}$ & 0.078 & -0.010\textcolor{white}{-} & 0.366 & 0.935 & 0.947 & -\\
\end{tabular}
\end{small}
\begin{tabular}{p{15cm}}
\hline\hline \\
\end{tabular}
\begin{tabulary}{15cm}{J}
\vspace*{-0.65cm}
{\scriptsize \textit{Notes:} The Table depicts the results of $ M = 1000$ Monte Carlo samples of data consisting of the responses of $200$ individuals observed $20$ times in each of two states. $\theta_{m}$ represents the true parameter in sample $m$ and $\hat{\theta}_{m}$ the corresponding estimate based on sample $m$. Bootstrapped standard errors based on 1000 samples. 
\par}
\end{tabulary}
\end{table}

Table \ref{tab:simulation stratEst} summarizes the results of the simulation exercise. Columns two to four depict the averages of the estimated parameters, the difference between the estimated to the true parameters, and the absolute difference between the estimated and the true parameters over the 1000 samples. The averages of the parameter estimates in the first column are close to the actual means of the distributions where the parameters are drawn from. The averages of the differences between the estimated and the drawn parameters in the second column show no systematic biases of the estimates. The averages of the absolute differences between the estimated and the true parameters are small. Note that the absolute differences are larger for the latent class regression coefficients due to the different scale of these parameters.  

Columns five and six of Table \ref{tab:simulation stratEst} report the coverage probabilities of analytic and bootstrapped of $95\%$ confidence intervals for the model parameters. The coverage probabilities are close to the expected probability of $0.95$. For the coverage probabilities displayed in rows one to four, the $95$ percent confidence interval based on 1000 Monte Carlo samples can be calculated which spans from $0.936$ to $0.964$.   

Next, the simulated data is used to illustrate the selection of the number of strategies, response probabilities, and trembles. Two additional strategies are added to the strategy matrix as red herrings which are similar to the two true underlying strategies. The augmented candidate set of four strategies offers several different possibilities to over-fit the data. To include the two additional strategies, \texttt{strategies} is augmented by the following rows:
\begin{equation}
\begin{bmatrix}  1 & 1 & 1 & 2  \\ 2 & \text{NA} & 1 & 2 \\  1 & \text{NA} & 1 & 2  \\ 2 & 0 & 1 & 2  \end{bmatrix} 
\end{equation}
In each of the $M$ samples, a selection of the number of strategies, responses and trembles based on the ICL criterion is executed with the command:\bigskip\\
\texttt{stratEst(data,strategies,covar,select ="all",crit = "ICL")}\bigskip\\
The last column of Table \ref{tab:simulation stratEst} depicts the results of the parameter selection. The numbers displayed in the last column indicate the frequency of selecting the correct number of shares, response probabilities and trembles across the $M$ samples. The first row indicates that the correct number of strategies selected in 999 out of 1000 samples. Rows three and four show that the probability to select the correct number of response parameters and trembles is substantially lower. This indicates that the selection of the number of responses and trembles frequently produces results generally over-estimate the number of responses and trembles. 


\section{Using the Package}
\label{sec:Using stratEst}
The central estimation function of the package is the function \texttt{stratEst()}. The function expects input objects  in the following order:\bigskip\\
\texttt{stratEst(data,strategies,shares,covariates,response,r.responses,r.trembles,}\\
\hspace*{2.5cm}\texttt{r.select,crit,se,outer.runs,outer.tol,outer.max,inner.runs,}\\
\hspace*{2.5cm}\texttt{inner.tol,inner.max,lcr.runs,lcr.tol,lcr.max)}\bigskip\\
Subsections \ref{subsec:Input objects} and \ref{subsec:Output objects} explain the input and the output objects of the function. 
\subsection{Input objects}
\label{subsec:Input objects}
\texttt{data:} Mandatory input object which contains the data for the estimation in the long format. Each row in \textsf{data} represents one observation of one individual. The object \textsf{data} must be an \textsf{R} data frame object with variables in columns. Three columns are mandatory: A column named \textsf{id} which identifies the observations of the same individual across the rows of the data frame. A column named \textsf{input} which indicates the type of information observed by the individual before giving a response. A column named \textsf{output} which contains the behavioral response of the individual after observing the input. If an individual plays the same game for more than one period with the same partner, \texttt{data} must contain a variable \textsf{period} which identifies the period within the game. If an individual plays the same game more than once with different partners, \texttt{data} must contain a variable \textsf{game} (or \texttt{supergame}) which identifies data from different games. For data from prisoner's dilemma experiments, two more data formats are possible. Instead of using the variables \textsf{input} and \textsf{output}, the data frame may also contain the variables \textsf{cooperation} and \textsf{other\textunderscore cooperation}, or alternatively, the variables  \textsf{cooperation} and \textsf{group}. The variable \textsf{cooperation} should be a dummy which indicates if the participant cooperated in the current period. The variable \textsf{other\textunderscore cooperation} should be a dummy which indicates if the other player cooperated in the current period. The variable \textsf{group} should be an identifier variable with a unique value for each unique match of two individuals. \smallskip\\
\texttt{strategies:} Mandatory input object. Can be either a positive integer or a matrix. If an integer is used, the estimation function will generate the respective number of memory-one strategies with as many states as there are unique input values in \textsf{data}. A matrix can be used to supply a set of customized strategies. In the matrix, each row corresponds to one state of a strategy, starting with the start state of an automaton. The first column enumerates the states of each strategy in ascending order. A value of one in the first column indicates the begin of a new strategy with its start state. The columns after the first column contain the collection of multinomial response vectors. The number of columns for the multinomial response vectors must correspond to the number of unique non-zero outputs in data. Without a reference output - which is labeled with a zero in the output column of data - the columns specify the complete multinomial response distribution for each unique value in the output column. In this case, the response probabilities in each row must sum to one. With a reference output, the response probability for the response labeled with zero is omitted and the response probabilities in each row must sum to a value smaller or equal to one. The remaining columns of the strategies matrix define the deterministic state transitions. The number of columns must equal the number of unique non-zero inputs in the data. The numbers in the first column indicate the next state of the automaton if the input is one. The numbers in the second column indicate the next state if the input is two and so on.\smallskip\\
\texttt{shares:} A column vector of strategy shares. The number of elements must correspond to the number of strategies defined in the strategies matrix. Elements which are NA are estimated from the data. If the object is not supplied, \textsf{stratEst} estimates a share for every strategy defined in the strategies matrix. \smallskip\\
\texttt{covariates:} A matrix where each row corresponds to same row in data. Hence, the covariate matrix must have as many rows as the data matrix. Observations which have the same ID in data must also have the same vector of covariates. Missing value are not allowed. If covariates are supplied, \textsf{stratEst} estimates the latent class regression model introduced in Section \ref{sec:Latent class regression}. \smallskip\\
\texttt{response:} String which can be set to \textit{pure} or \textit{mixed}. If set to \textit{pure} all response probabilities estimated from the data are pure responses. If set to \textit{mixed} all response probabilities estimated from the data are mixed responses. The default is \textit{mixed}.\smallskip\\
\texttt{r.responses:} A string which can be set to \textit{no}, \textit{strategies}, \textit{states} or \textit{global}. If set to \textit{strategies}, the estimation function estimates strategies with one strategy specific vector of responses in every state of the strategy. If set to \textit{states}, one state specific vector of responses is estimated for each state. If set to \textit{global}, a single vector of responses is estimated which applies in every state of each strategy. Default is \textit{no}.\smallskip\\
\texttt{r.trembles:} String which can be set to \textit{no}, \textit{strategies}, \textit{states} or \textit{global}. If set to \textit{strategies}, the estimation unction estimates strategies with one strategy specific tremble probability. If set to  \textit{states}, one state specific tremble probability is estimated for each state. If set to \textit{global}, a single tremble is estimated which applies in every state of each strategy. Default is \textit{no}.\smallskip\\
\texttt{select:} String which can be set to \textit{no}, \textit{strategies}, \textit{responses}, \textit{trembles}, \textit{both}, and \textit{all}. If set to \textit{strategies}, \textit{responses}, \textit{trembles}, the number of strategies, responses, trembles respectively are selected based on the selection criterion specified in option \textit{crit}. If set to \textit{both}, the number of responses and trembles are selected. If set to \textit{all}, the number of strategies, responses, and trembles are selected. Default is \textit{no}.\smallskip\\
\texttt{crit:} String which can be set to \textit{BIC}, \textit{AIC} or \textit{ICL}. If set to \textit{BIC}, model selection based on the Bayesian Information criterion is performed. If set to AIC, the Akaike Information criterion is used. If set to \textit{ICL} the Integrated Classification Likelihood criterion is used. Default is \textit{BIC}.\smallskip\\
\texttt{se:} String which can be set to \textit{no}, \textit{yes} or \textit{bs}. If set to \textit{no}, standard errors are not reported. If set to \textit{yes}, analytic standard errors are reported. If set to \textit{bs}, bootstrapped standard errors are reported for responses and trembles. Default is \textit{yes}.\smallskip\\
\texttt{outer.runs:} Positive integer which stets the number of outer runs of the solver. Default is 10.\smallskip\\
\texttt{outer.tol:} Positive number which stets the tolerance of the continuation condition of the outer runs. The iterative algorithm stops after iteration $j$ if $1 - LL_{j}/LL_{j-1} < $ outer.tol. Default is 0.\smallskip\\
\texttt{outer.max:} Positive integer which stets the maximum number of iterations of the outer runs of the solver. The iterative algorithm stops after iteration $j$ if $j = $ outer.max. Default is 1000.\smallskip\\
\texttt{inner.runs:}  Positive integer which stets the number of inner runs of the solver. Default is 100.\smallskip\\
\texttt{inner.tol:} Positive number which stets the tolerance of the continuation condition of the inner EM runs. The iterative algorithm stops after iteration $j$ if $1 - LL_{j}/LL_{j-1} < $ inner.tol. Default is 0.\smallskip\\
\texttt{inner.max:} Positive integer which stets the maximum number of iterations of the inner EM runs. The iterative algorithm stops after iteration $j$ if $j = $ inner.max. Default is 100.\smallskip\\
\texttt{lcr.runs:} Positive integer which stets the number of estimation runs for latent class regression. Default is 100.\smallskip\\
\texttt{lcr.tol:} Positive number which stets the tolerance of the continuation condition of the latent class regression runs. The iterative algorithm stops after iteration $j$ if $1 - LL_{j}/LL_{j-1} < $ LCR.tol. Default is 0.\smallskip\\
\texttt{lcr.max:} Positive integer which stets the maximum number of iterations of the latent class regression EM runs. The iterative algorithm stops after iteration $j$ if $j = $ inner.max. Default is 1000.\smallskip\\
%\textbf{stepsize:} Value between zero and one adjusting the stepsize of the Newton-Raphson algorithm which is used to update the coefficients of a Latent Class regression model. Reducing the step size can prevent singular Fisher Information matrices. Caution is required when using this option since it has not been thoroughly tested. Default is 1.\smallskip\\
%\textbf{penalty:} Penalty term to obtain maximum a posteriori (MAP) estimates of Latent Class Regression coefficients with normal priors with mean zero. Penalty is the reciprocal of the variance of the prior distribution. Values greater than zero punish extreme values of the regression coefficients and shift MAP estimates towards zero. The option can be used to avoid boundary estimates under quasi-complete separation. Only use if coefficients take extreme values and try small numbers (e.g. 0.1). Default is 0. \smallskip\\

\subsection{Output objects}
\label{subsec:Output objects}
\texttt{shares:} Column vector which contains the estimates of population shares for the strategies. The first element corresponds to the first strategy defined in the strategy matrix, the second element to corresponds to the second strategy and to on. Can be used as input object of the estimation function.\smallskip\\
\texttt{strategies:} Matrix which contains the strategies of the model. Can be used as input object of the of the estimation function. \smallskip\\
\texttt{responses:} Column vector which contains the response probabilities of the strategies. The value -1 indicates that the corresponding response could not be estimated since data does not contain observations the model assigns to the corresponding state. \smallskip\\
\texttt{trembles:} Column vector which contains the tremble probabilities of the strategies. The value -1 indicates that the corresponding response could not be estimated since data does not contain observations the model assigns to the corresponding state.\smallskip\\
\texttt{coefficients:} Column vector which contains the latent class regression coefficients for strategies 2 to $k$.\smallskip\\
\texttt{response.mat:} Matrix which contains the estimates of the response probabilities for the columns of the strategy matrix which represent the response probabilities.\smallskip\\
\texttt{tremble.mat:} Matrix which contains the estimates of the tremble probabilities for the columns of the strategy matrix which represent the response probabilities.\smallskip\\
\texttt{coefficient.mat:} Matrix which contains the latent class regression coefficients of strategies 2 to $K$ in columns.\smallskip\\
\texttt{ll.val:} The log-Likelihood value corresponding to the reported estimates. Bigger values indicate a better fit of the model to the data.\smallskip\\
\texttt{crit.val:} The value of the selection criterion defined with option \textit{crit}. Bigger values indicate a better fit of the model.\smallskip\\
\texttt{eval:} Number of iterations of the solver. The reported number is the sum of iterations performed in the inner and the outer run which led to the reported estimates.\smallskip\\
\texttt{tol.val:} The tolerance value in the last iteration.\smallskip\\
\texttt{assignments:} Matrix which contains the posterior probability assignments $\theta_{ik}$ of individuals to strategies. The matrix has $N$ rows which correspond to the ID sorted in ascending order beginning with the individual with the lowest ID. The matrix has $K$ columns which correspond to the strategies, starting with the first strategy defined in the strategy matrix in column one. \smallskip\\
\texttt{priors:}  Matrix which contains the individual prior probabilities $p_{ik}$ of individuals as predicted by the covariate vectors of the individuals. The matrix has $N$ rows which correspond to the ID sorted in ascending order beginning with the individual with the lowest ID. The matrix has $K$ columns which correspond to the strategies, starting with the first strategy defined in the strategy matrix. \smallskip\\
\texttt{shares.se:} Column vector which contains the standard errors of the estimated shares. The elements correspond to the vector of estimates.\smallskip\\
\texttt{responses.se:} Column vector which contains the standard errors of the reported responses. The elements correspond to the vector of estimates.\smallskip\\
\texttt{trembles.se:} Column vector which contains the standard errors of the reported trembles. The elements correspond to the vector of estimates. \smallskip\\
\texttt{coefficients.se:} Column vector which contains the standard errors of the reported coefficients.  The elements correspond to the vector of estimates. \smallskip\\
\texttt{convergence:} Row vector which reports the maximum value of the score vector of the shares as the first element, responses as the second element, trembles as the third element, and LCR coefficients as the forth element. Small values indicate convergence of the algorithm to a (local) maximum.  \smallskip\\

%\subsection{Further Analyses of DalBo \& Frechette, 2011}
%\label{subsec:Example 2}
%This subsection illustrates the use of the stratEst package based on data from \cite{DalBo2011}. Strategy estimation is performed for a candidate set of six strategies. It is shown how to use strategy selection to obtain solutions with fewer strategies. It is also shown how the evolution of strategy choices over time can be analyzed based on latent class regression models. The data contains the binary decisions of $266$ individuals playing the indefinitely repeated prisoner's dilemma after a learning phase of 110 interactions. The data can be loaded with the command \texttt{load(\text{DF2011})}. The variable output in column five is $1$ if the action was cooperation, and $2$ if the action was defection. \cite{DalBo2011} report strategy estimation results for six different treatments which arise from combinations of two different continuation probabilities of $\delta = 1/2$ and $\delta = 2/3$ and three different payoffs for mutual cooperation $R = 32$, $R = 40$, and $R = 48$. Table 7 on page 424 of \cite{DalBo2011} reports the results of treatment-wise strategy estimations for a candidate set of the six strategies. The candidate set includes the strategies: Always Defect (ALLD), Always Cooperate (ALLC), Tit-for-Tat (TFT), Grim-trigger (GRIM), Win-stay-lose-shift (WSLS), and a trigger strategy with two periods of punishment (T2). The six strategies are contained in the set of prisoner's dilemma game strategies which can be loaded with \texttt{load(PD)}. After loading the data and the strategy set, the input objects of the estimation function for treatment $t\in \{1,\cdots,6\}$ can be generated by:\bigskip\\ \texttt{data = DF2011[DF2011[,1]==t,2:6]}\bigskip\\
%\texttt{strategies = rbind(\text{ALLD,ALLC,GRIM,TFT,WSLS,T2})}\bigskip\\
%The treatment specific estimates reported in \cite{DalBo2011} are reproduced with the command:\bigskip\\
%\texttt{stratEst(data,strategies)}\bigskip\\
%The results show that in each of the six treatments, the maximum-likelihood shares of at least two strategies are zero. A selection of the number of strategies can be used to obtain a condensed set of strategies for each treatment. The treatment-wise maximum-likelihood estimation with strategy selection is performed with the command:\bigskip\\
%\texttt{stratEst(data,strategies,select="strategies",crit="AIC")}\bigskip\\ 
%Table \ref{tab:relication DalBo} depicts the results of the treatment-wise execution with strategy selection. The strategy selection procedure eliminates all strategies with zero shares reported in \cite{DalBo2011}. In two of the six treatments, it also eliminates strategies which attract positive shares without strategy selection. In column three, the selection procedure assigns the share of $0.019$ of WSLS reported in the original paper to TFT. In column six, the share of $0.116$ for GRIM is distributed among TFT and T2. Note that the Akaike selection criterion implements the mildest penalty among all three selection criteria. BIC, and ICL criteria will produce results with fewer strategies. The bootstrapped standard errors reported in parentheses are generally smaller than the standard errors reported in \cite{DalBo2011}. Analytic standard errors are similar to those reported in Table \ref{tab:relication DalBo}.
%\begin{table}[htbp]
%	\caption{Replication of Table 7 in \cite{DalBo2011}}
%	\label{tab:relication DalBo}
%	\setlength{\tabcolsep}{12pt}
%	\centering 	
%\begin{small}
%		\begin{tabular}{p{15cm}}
%			\hline\hline \\    [-2.5ex]
%		\end{tabular}
%		\begin{tabular}{lccccccc}
%	& \multicolumn{3}{c}{$\delta=1/2$} & & \multicolumn{3}{c}{$\delta=3/4$} \\ [0.5ex]
%		\cline{2-4}  \cline{6-8} \\[-2.5ex]
% & $R = 32$ & $R = 40$ & $R = 48$ & & $R = 32$ & $R = 40$ & $R = 48$\\ 
%\midrule
%ALLD & 0.920 & 0.783 & 0.533 & & 0.648 & 0.109 & -\\
%& (0.043) & (0.060)& (0.075) & & (0.074) & (0.052) & -\\
%ALLC & - & 0.078 & 0.072  & & - & 0.296 & 0.076 \\
%& -  & (0.041)& (0.039 & & - & (0.117) & (0.075)\\
%GRIM & - & 0.040 & - & & - & 0.267 & -\\
%& - & (0.029)& - & & - & (0.118) & -\\
%TFT & 0.080 & 0.098 & 0.395 & & 0.352 & 0.327 & 0.606\\
%& (0.043) &  (0.047) & (0.074) & & (0.074) & (0.137) & (0.148)\\
%WSLS & - & -& -  & & - & - & -\\
%& - & - & - & & - & - & -\\
%T2 & -& - & - & & - & 0.319 & -\\
%& - & - & - & & - & (0.148) & -\\
%\midrule
%$\gamma$ & 0.060 & 0.136 & 0.090 & & 0.096 & 0.091 & 0.033\\
%& (0.020) & (0.022) & (0.020) & & (0.019) & (0.030) & (0.012)\\
%\end{tabular}
%\end{small}
%\begin{tabular}{p{15cm}}
%\hline\hline \\
%\end{tabular}
%\begin{tabulary}{15cm}{J}
%\vspace*{-0.65cm}
%{\scriptsize \textit{Notes:} The Table depicts the results of a treatment-wise estimation of the candidate set used in \cite{DalBo2011} with strategy selection based on ICL. Estimates are reported for all strategies which survive the selection procedure. Bootstrapped standard errors based on 1000 samples in parentheses.
%\par}
%\end{tabulary}
%\end{table}
%
%Figure $1$ on page $419$ of \cite{DalBo2011} suggests that participants learn and change strategies in the first supergames until strategy use becomes individually stable after 110 interactions. \cite{DalBo2011} analyze the role of experience on strategy choices by restricting their attention to the two strategies ALLD and TFT. They abstract from the complexities of the repeated game by focusing on behavior in period 1 - where ALLD prescribes defection and TFT prescribes cooperation. Figure $1$ on page $419$ of \cite{DalBo2011} shows that learning model predicts that individuals adopt ALLD in treatments with $\delta=1/2$ and TFT in treatments with $\delta=3/4$ over the course of the first 20 supergames. Latent class regression can be used to extend the analysis of the role of experience for strategy choices. The treatment data of the first 20 supergames of \cite{DalBo2011} can be loaded with the command \texttt{load(DF2011LCR)}. The role of experience for strategy choices can be analyzed by assuming that strategy choices are realizations of a distribution which changes over time. To implement this assumption, the data set contains a unique ID for every participant in every supergame. Strategy choices are analyzed for the complete set of six strategies. It is useful to select the number of strategies when performing latent class regression to avoid strategies with shares of zero which produce infinite regression coefficients. The prior probability distribution is model by an intercept and the supergame number. For the estimation for treatment $t\in\{1,\cdots,6\}$, the following series of commands is executed to generate the input objects for treatment $t$:\bigskip\\ \texttt{data = DF2011LCR[DF2011LCR[,1]==t,2:6]}\bigskip\\
%\texttt{covariate = matrix(data[,2]-1,length(data[,2],1)}\bigskip\\
%\texttt{data[,2] = rep(1,nrows(data))}\bigskip\\
%\texttt{strategies = rbind(\text{ALLD,ALLC,GRIM,TFT,WSLS,T2})}\bigskip\\
%The latent class regression model for treatment $t$ is estimated with the following command:\bigskip\\
%\texttt{stratEst(data,strategies,covariate,select="strategies",crit="ICL")}\bigskip\\
%\begin{table}[htbp]
%\caption{Experience as a covariate for strategy choices}
%\label{tab:LCR DalBo}
%\centering 	
%\setlength{\tabcolsep}{3.5pt}
%\begin{small}
%		\begin{tabular}{p{15cm}}
%			\hline\hline \\     [-2.5ex]
%		\end{tabular}
%		\begin{tabular}{llccccccccccc}
%%&		& \multicolumn{11}{c}{$\delta = 1/2$}\\ [0.5ex]
%%	\cline{3-13} \\[-2ex]
%&  & \multicolumn{3}{c}{$R= 32$} & &  \multicolumn{3}{c}{$R = 40$}  & & \multicolumn{3}{c}{$R = 48$} \\ [0.5ex]
%		\cline{3-5}  \cline{7-9} \cline{11-13} \\[-1.5ex]
%$\delta$ &  & $p_{k}$ & $\beta_{0}$ & $\beta_{sg}$ & & $p_{k}$ & $\beta_{0}$ & $\beta_{sg}$ & & $p_{k}$ & $\beta_{0}$ & $\beta_{sg}$\\ 
%\midrule
%$1/2~~~$& ALLD & 0.935 & - & -  & &  0.916 & - & -  & & 0.715 & - & - \\
%&      & (0.012) & - & -  & & (0.013) & - & - & & (0.019) & - & -  \\
%& TFT & 0.065 & -1.705 & -0.126  & & - & - & -  & & - & - & - \\
%& & (0.012) & (0.154) & (0.021)  & & - & - & -  & & - & - & - \\
%& ALLC & - & - & -  & & 0.084 & -0.553 & -0.304  & & 0.285 & -0.765 & -0.016 \\
%& & - & - & -  & & (0.013) & (0.129) & (0.032)  & & (0.019) & (0.086) & (0.008) \\
%%&              $\gamma$ & 0.106 & - & - & & 0.140 & - & - & & 0.118 & - & -  \\
%%&              & (0.007) & - & - & & (0.009) & - & - & & (0.004) & - & - \\
%\midrule
%$3/4$ & ALLD & 0.851 & - & -  & &  0.530 & - & -  & & 0.286 & - & - \\
%& & (0.014) & - & -  & & (0.019) & - & - & & (0.017) & - & -  \\
%& TFT & - & - & -  & & - & - & -  & & 0.049 & -1.201 & -0.106 \\
%& & - & - & -  & & - & - & -  & & (0.019) & (0.302) & (0.048) \\
%& ALLC & 0.149 & -2.168 & 0.043  & & 0.568 & -1.252 & 0.167  & & 0.665 & -0.612 & 0.166 \\
%& & (0.014) & (0.131) & (0.011)  & & (0.019) & (0.092) & (0.009)  &  & (0.015) & (0.192) & (0.026) \\
%%&              $\gamma$ & 0.106 & - & - & & 0.128 & - & - & & - & - & -  \\
%%&              & (0.004) & - & - & & (0.005) & - & - & & - & - & - \\
%\end{tabular}
%\end{small}
%\begin{tabular}{p{15cm}}
%\hline\hline \\
%\end{tabular}
%\begin{tabulary}{15cm}{J}
%\vspace*{-0.65cm}
%{\scriptsize \textit{Notes:} The Table depicts the results of latent class regression models with the supergame number as covariate. Estimates for supergames 1-20. Strategies are selected based on $ICL$. Estimates are reported for strategies which survive the selection procedure in at least one of the treatments. Analytic standard errors based on 1000 samples in parentheses.
%\par}
%\end{tabulary}
%\end{table}
%Table \ref{tab:LCR DalBo} depicts the estimates of the latent class regression models. The latent class models characterize the behavior of the participants in the first 20 supergames based on the three simple strategies ALLD, TFT, and ALLC. The strategy ALLD attracts a substantial share in each of the six treatments. Since the covariate is \texttt{supergame - 1}, the coefficients of the intercepts $\beta_{0}$ indicate the prevalence of the cooperative strategies as compared to ALLD in the first supergame. The coefficients $\beta_{sg}$ reflect the mean increase in the logarithm of the odds-ratio between ALLD and the cooperative strategies. 
%
%To give an example, in the treatment with $R=32$ and $ \delta=1/2$, the prevalence of ALLD in the first supergame is $1/(1 + e^{-1.705}) = 0.846$.  In supergame 20, the prevalence of ALLD has increased to $1/(1 + e^{-1.705 - 19 \times 0.126}) = 0.987$. The odds ratio of playing TFT instead of ALLD is $0.166$ in supergame 1  and decreases to $0.017$ in supergame 20. The coefficient $\beta_{sg}$ indicated an average decreases of the odds ratio to play TFT by the factor $e^{-0.126} = 0.882$ from one supergame to the next. The latent class regression coefficients reported in Table \ref{tab:LCR DalBo} reveal that participants switch to ALLD in the treatments with $\delta=1/2$ and to cooperative strategies in the treatments with  $\delta=3/4$ over the course of the first 20 supergames. The reported standard errors indicate that the observed time trends in strategy choices are statistically significant. 

\begin{small}
\bibliographystyle{ecta}
%\bibliography{C:/users/anon/ownCloud/private/Projects/stratEstPackage/vignette/stratEst_vignette}
\begin{thebibliography}{35}
\newcommand{\enquote}[1]{``#1''}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[\protect\citeauthoryear{Agresti}{Agresti}{2003}]{Agresti2003}
\textsc{Agresti, A.} (2003): \emph{Logit Models for Multinomial Responses},
  Wiley-Blackwell, chap.~7, 267--313.

\bibitem[\protect\citeauthoryear{Akaike}{Akaike}{1973}]{Akaike1973}
\textsc{Akaike, H.} (1973): \emph{Second International Symposium on Information
  Theory}, Budapest, Hungary: Akademiai Kiado, chap. Information Theory and an
  Extension of the Maximum Likelihood Principle, 267--281.

\bibitem[\protect\citeauthoryear{Aoyagi, Bhaskar, and Fr{\'{e}}chette}{Aoyagi
  et~al.}{2017}]{Aoyagi2017}
\textsc{Aoyagi, M., V.~Bhaskar, and G.~R. Fr{\'{e}}chette} (2017):
  \enquote{{The Impact of Monitoring in Infinitely Repeated Games: Perfect,
  Public, and Private},} \emph{Mimeo}.

\bibitem[\protect\citeauthoryear{Aoyagi and Fr{\'{e}}chette}{Aoyagi and
  Fr{\'{e}}chette}{2009}]{Aoyagi2009}
\textsc{Aoyagi, M. and G.~R. Fr{\'{e}}chette} (2009): \enquote{{Collusion as
  public monitoring becomes noisy: Experimental evidence},} \emph{Journal of
  Economic Theory}, 144, 1135--1165.

\bibitem[\protect\citeauthoryear{Arechar, Dreber, Fudenberg, and Rand}{Arechar
  et~al.}{2017}]{Arechar2017}
\textsc{Arechar, A.~A., A.~Dreber, D.~Fudenberg, and D.~G. Rand} (2017):
  \enquote{I'm just a soul whose intentions are good?: The role of
  communication in noisy repeated games,} \emph{Games and Economic Behavior},
  104, 726--743.

\bibitem[\protect\citeauthoryear{Bandeen-Roche, Miglioretti, Zeger, and
  Rathouz}{Bandeen-Roche et~al.}{1997}]{Bandeen1997}
\textsc{Bandeen-Roche, K., D.~L. Miglioretti, S.~L. Zeger, and P.~J. Rathouz}
  (1997): \enquote{Latent Variable Regression for Multiple Discrete Outcomes,}
  \emph{Journal of the American Statistical Association}, 92, 1375--1386.

\bibitem[\protect\citeauthoryear{Beath}{Beath}{2011}]{Beath2011}
\textsc{Beath, K.} (2011): \enquote{randomLCA: Random Effects Latent Class
  Analysis,} Tech. rep., R package version 0.7, URL
  http://CRAN.R-project.org/package=randomLCA.

\bibitem[\protect\citeauthoryear{Bengtsson}{Bengtsson}{2018}]{Bengtsson2018}
\textsc{Bengtsson, H.} (2018): \emph{R.rsp: Dynamic Generation of Scientific
  Reports}, r package version 0.43.0.

\bibitem[\protect\citeauthoryear{Biernacki, Celeux, and Govaert}{Biernacki
  et~al.}{2000}]{Biernacki2000}
\textsc{Biernacki, C., G.~Celeux, and G.~Govaert} (2000): \enquote{{Assessing a
  mixture model for clustering with the integrated completed likelihood},}
  \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 22,
  719--725.

\bibitem[\protect\citeauthoryear{Biernacki, Celeux, and Govaert}{Biernacki
  et~al.}{2003}]{Biernacki2003}
---\hspace{-.1pt}---\hspace{-.1pt}--- (2003): \enquote{{Choosing starting
  values for the {\{}EM{\}} algorithm for getting the highest likelihood in
  multivariate Gaussian mixture models},} \emph{Computational Statistics {\&}
  Data Analysis}, 41, 561--575.

\bibitem[\protect\citeauthoryear{Bolck, Croon, and Hagenaars}{Bolck
  et~al.}{2004}]{Bolck2004}
\textsc{Bolck, A., M.~Croon, and J.~Hagenaars} (2004): \enquote{Estimating
  Latent Structure Models with Categorical Variables: One-Step Versus
  Three-Step Estimators,} \emph{Political Analysis}, 12, 3--27.

\bibitem[\protect\citeauthoryear{Breitmoser}{Breitmoser}{2015}]{Breitmoser2015}
\textsc{Breitmoser, Y.} (2015): \enquote{{Cooperation, but no reciprocity:
  Individual strategies in the repeated prisoner's dilemma},} \emph{American
  Economic Review}, 105, 2882--2910.

\bibitem[\protect\citeauthoryear{Camera, Casari, and Bigoni}{Camera
  et~al.}{2012}]{Camera2012}
\textsc{Camera, G., M.~Casari, and M.~Bigoni} (2012): \enquote{{Cooperative
  strategies in anonymous economies: An experiment},} \emph{Games and Economic
  Behavior}, 75, 570--586.

\bibitem[\protect\citeauthoryear{{Dal B{\'{o}}} and Fr{\'{e}}chette}{{Dal
  B{\'{o}}} and Fr{\'{e}}chette}{2011}]{DalBo2011}
\textsc{{Dal B{\'{o}}}, P. and G.~R. Fr{\'{e}}chette} (2011): \enquote{{The
  evolution of cooperation in infinitely repeated games: Experimental
  evidence},} \emph{American Economic Review}, 101, 411--429.

\bibitem[\protect\citeauthoryear{Dayton and Macready}{Dayton and
  Macready}{1988}]{Dayton1988}
\textsc{Dayton, C.~M. and G.~B. Macready} (1988): \enquote{Concomitant-Variable
  Latent-Class Models,} \emph{Journal of the American Statistical Association},
  83, 173--178.

\bibitem[\protect\citeauthoryear{Dempster, Laird, and Rubin}{Dempster
  et~al.}{1977}]{Dempster1977}
\textsc{Dempster, A., N.~Laird, and D.~B. Rubin} (1977): \enquote{{Maximum
  likelihood from incomplete data via the EM algorithm},} \emph{Journal of the
  Royal Statistical Society Series B}, 39, 1--38.

\bibitem[\protect\citeauthoryear{Dvorak and Fehrler}{Dvorak and
  Fehrler}{2018}]{Dvorak2018b}
\textsc{Dvorak, F. and S.~Fehrler} (2018): \enquote{Negotiating Cooperation
  Under Uncertainty: Communication in Noisy, Indefinitely Repeated
  Interactions,} Tech. rep., IZA Discussion Paper No. 11897.

\bibitem[\protect\citeauthoryear{Eddelbuettel and Fran\c{c}ois}{Eddelbuettel
  and Fran\c{c}ois}{2011}]{Eddelbuettel2011}
\textsc{Eddelbuettel, D. and R.~Fran\c{c}ois} (2011): \enquote{{Rcpp}: Seamless
  {R} and {C++} Integration,} \emph{Journal of Statistical Software}, 40,
  1--18.

\bibitem[\protect\citeauthoryear{Embrey, Fr{\'{e}}chette, and
  Stacchetti}{Embrey et~al.}{2013}]{Embrey2013}
\textsc{Embrey, M., G.~Fr{\'{e}}chette, and E.~Stacchetti} (2013): \enquote{{An
  Experimental Study of Imperfect Public Monitoring: Efficiency Versus
  Renegotiation-Proofness},} \emph{Mimeo}.

\bibitem[\protect\citeauthoryear{Fudenberg, Rand, and Dreber}{Fudenberg
  et~al.}{2012}]{Fudenberg2012}
\textsc{Fudenberg, D., D.~G. Rand, and A.~Dreber} (2012): \enquote{{Slow to
  Anger and Fast to Forgive: Cooperation in an Uncertain World},}
  \emph{American Economic Review}, 102, 720--749.

\bibitem[\protect\citeauthoryear{Kaufman and Rousseeuw}{Kaufman and
  Rousseeuw}{1990}]{Kaufman1990}
\textsc{Kaufman, L. and P.~J. Rousseeuw} (1990): \emph{Finding groups in data.
  an introduction to cluster analysis}, Wiley Series in Probability and
  Mathematical Statistics. Applied Probability and Statistics, New York: John
  Wiley \& Sons, Inc.

\bibitem[\protect\citeauthoryear{King and Roberts}{King and
  Roberts}{2015}]{King2015}
\textsc{King, G. and M.~E. Roberts} (2015): \enquote{How Robust Standard Errors
  Expose Methodological Problems They Do Not Fix, and What to Do About It,}
  \emph{Political Analysis}, 23, 159--179.

\bibitem[\protect\citeauthoryear{Lazarsfeld}{Lazarsfeld}{1950}]{Lazarsfeld1950}
\textsc{Lazarsfeld, P.~F.} (1950): \emph{Measurement and Prediction}, New York:
  John Wiley \& Sons, Inc., chap. The Logical and Mathematical Foundations of
  Latent Structure Analysis, 362--412.

\bibitem[\protect\citeauthoryear{Leisch}{Leisch}{2004}]{Leisch2004}
\textsc{Leisch, F.} (2004): \enquote{FlexMix: A General Framework for Finite
  Mixture Models and Latent Class Regression in R,} \emph{Journal of
  Statistical Software}, 11.

\bibitem[\protect\citeauthoryear{Linzer and Lewis}{Linzer and
  Lewis}{2011}]{Linzer2011}
\textsc{Linzer, D.~A. and J.~B. Lewis} (2011): \enquote{poLCA: An R Package for
  Polytomous Variable Latent Class Analysis,} \emph{Journal of Statistical
  Software}, 42.

\bibitem[\protect\citeauthoryear{McLachlan and Peel}{McLachlan and
  Peel}{2005}]{McLachlan2005}
\textsc{McLachlan, G. and D.~Peel} (2005): \emph{Finite Mixture Models}, New
  York: John Wiley \& Sons, Inc.

\bibitem[\protect\citeauthoryear{Meilijson}{Meilijson}{1989}]{Meilijson1989}
\textsc{Meilijson, I.} (1989): \enquote{A Fast Improvement to the EM Algorithm
  on its Own Terms,} 51, 127--138.

\bibitem[\protect\citeauthoryear{R${~}$Development${~}$Core${~}$Team}{R${~}$Development${~}$Core${~}$Team}{2008}]{R2008}
\textsc{R${~}$Development${~}$Core${~}$Team} (2008): \enquote{R: A language and
  environment for statistical computing,} Tech. rep., R Foundation for
  Statistical Computing.

\bibitem[\protect\citeauthoryear{Sanderson and Curtin}{Sanderson and
  Curtin}{2016}]{Sanderson2016}
\textsc{Sanderson, C. and R.~Curtin} (2016): \enquote{Armadillo: a
  template-based C++ library for linear algebra.} \emph{Journal of Open Source
  Software}, 1, 26.

\bibitem[\protect\citeauthoryear{Schwarz}{Schwarz}{1978}]{Schwarz1978}
\textsc{Schwarz, G.} (1978): \enquote{Estimating the Dimension of a Model,}
  \emph{Ann. Statist.}, 6, 461--464.

\bibitem[\protect\citeauthoryear{Selten}{Selten}{1975}]{Selten1975}
\textsc{Selten, R.} (1975): \enquote{Reexamination of the perfectness concept
  for equilibrium points in extensive games,} \emph{International Journal of
  Game Theory}, 4, 25--55.

\bibitem[\protect\citeauthoryear{Wickham}{Wickham}{2011}]{Wickham2011}
\textsc{Wickham, H.} (2011): \enquote{testthat: Get Started with Testing,}
  \emph{The R Journal}, 3, 5--10.

\bibitem[\protect\citeauthoryear{Wickham, Danenberg, and Eugster}{Wickham
  et~al.}{2018{\natexlab{a}}}]{Wickham2018b}
\textsc{Wickham, H., P.~Danenberg, and M.~Eugster} (2018{\natexlab{a}}):
  \emph{roxygen2: In-Line Documentation for R}, r package version 6.1.1.

\bibitem[\protect\citeauthoryear{Wickham, Hester, and Chang}{Wickham
  et~al.}{2018{\natexlab{b}}}]{Wickham2018a}
\textsc{Wickham, H., J.~Hester, and W.~Chang} (2018{\natexlab{b}}):
  \emph{devtools: Tools to Make Developing R Packages Easier}, r package
  version 2.0.1.

\bibitem[\protect\citeauthoryear{Xie}{Xie}{2018}]{Xie2018}
\textsc{Xie, Y.} (2018): \enquote{knitr: A General-Purpose Package for Dynamic
  Report Generation in R,} Tech. rep., R package version 1.20.

\end{thebibliography}
\end{small}
%

\newpage 

\setcounter{section}{0}
\renewcommand{\thesection}{Appendix}

\setcounter{figure}{0}
\renewcommand{\thefigure}{\Alph{section}.\arabic{figure}}

\setcounter{table}{0}
\renewcommand{\thetable}{\Alph{section}.\arabic{table}}

\section{}
\label{sec:appendix}
Tables \ref{tab:PD_set1}, \ref{tab:PD_set2}, and \ref{tab:PD_set3} depict 22 strategies for the repeated prissonne's dilemma which can be used after the package is loaded and attached to the search path with the command \texttt{library(stratEst)}. Strategies 1-20 and their descriptions are taken from \cite{Fudenberg2012}. Strategy 21 is the semi-grim structure discovered by \cite{Breitmoser2015}. In the automata representations in column three, circles represent strategy states and arrows transitions between strategy states. The start state $s_{k0}$ of the strategies is always the first circle from the left. Capital letters in the circles indicates the action the automata prescribes in the state. Variables indicate an ex-ante unspecified probability to cooperate. Transitions between states occur conditional on the action profile of the current period. Letters next to transition arrows indicate that the transition occurs conditional on observing this profile. The first letter of action profiles indicates the own action and the second letter the action of the other player in the current period. To give an example, if the action profile next to the arrow is $cd$, the transition arrow is applies if the own action i $c$ and the action of the other player i $d$ in the current period. If no action profile is depicted next to an arrow, the transition arrow applies unconditionally, for all possible action profiles which can be observed.

The strategies depicted in Tables \ref{tab:PD_set1}, \ref{tab:PD_set2}, and \ref{tab:PD_set3}, can be used for data which has the following format: The output in column five must be 1 if the action of the player was cooperation and 0 if the action was defection. Zeros have to be used as inputs in period one. In all other periods the values 1, 2, 3 and 4 are used to indicate the strategy profiles $cc$, $cd$, $dc$ and $dd$ of the current period respectively. The matrix representation of strategies follows Section \ref{sec: Deterministic finite-state automata}. Each row represents one state of a strategy and the first column indicates the state number. The second column the probability to play $C$ in every state. Columns 3-6 indicate the deterministic state transitions after observing the action profiles $cc$, $cd$, $dc$, and $dd$ respectively.



\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse,fill=blue!0, node distance=1cm,minimum height=2em]
\begin{table}[htbp]
\caption{Pre-programmed prisoner's dilemma strategies (1-10)}
\label{tab:PD_set1}
\setlength{\tabcolsep}{5pt}
	\renewcommand\arraystretch{1}
	\centering
%	\caption{Set of strategies adopted from Fudenberg et al.}
\begin{footnotesize}
	\begin{tabular}{p{1.5cm}p{4.5cm}cc}
							\hline
						\hline\\[-1.5ex]
		{\small Acronym} & {\small Description } & {\small Automaton} & {\small  Matrix} \\[0.5ex]
	& & & $~s ~~~ \pi^{c} ~ cc ~~ cd ~~ dc ~~ dd$\\ 
%		& & & {\tiny $\begin{matrix} s & \pi^{c}_{s} & \phi_{cc} & \phi_{cd} & \phi_{dc} & \phi_{dd} \end{matrix}$ } \\
		\hline\\ [-1.2ex]   
		ALLD & \begin{tabular}{p{4.5cm}} Always play D \end{tabular} & 
	\begin{tikzpicture}[baseline=(C.base),node distance = 1cm, auto]
    \node [cloud] (D) {D};
    \node[fill=white] at (0,-0.8) (Photo) {{\tiny }};
    \draw [->] (D) to [out=225,in=315,looseness=3] (D);
   \end{tikzpicture}
    & $\begin{matrix} 1 & 0 & 1 & 1 & 1 & 1  \\ \end{matrix}$ \\
    		ALLC & \begin{tabular}{p{4.5cm}}Always play C \end{tabular}  & 
	\begin{tikzpicture}[baseline=(C.base),node distance = 1cm, auto]
    \node [cloud] (C) {C};
    \node[fill=white] at (0,-0.8) (Photo) {{\tiny }};
    \draw [->] (C) to [out=225,in=315,looseness=3] (C);
   \end{tikzpicture}
    & $\begin{matrix} 1 & 1 & 1 & 1 & 1 & 1  \\ \end{matrix}$ \\
		DC &  \begin{tabular}{p{4.5cm}}Start with D, then alternate between C and D  \end{tabular} & 
			\begin{tikzpicture}[baseline=(C.base),node distance = 1cm, auto]
    \node [cloud] (D) {D};
    \node[fill=white] at (0.5,0.6) (Photo) {{\tiny }};
        \node[fill=white] at (0.5,-0.6) (Photo) {{\tiny }};
    \node [cloud, right of=D] (C) {C};
    \draw [->] (D) to [out=45,in=135] (C);
    \draw [->] (C) to [out=-135,in=-45] (D);
   \end{tikzpicture} 
   & $\begin{matrix} 1 & 0 & 2 & 2 & 2 & 2  \\ 2 & 1 & 1 & 1 & 1 & 1  \end{matrix}$ \\
		FC & \begin{tabular}{p{4.5cm}}Play C in the first period, then D forever  \end{tabular} & 
				\begin{tikzpicture}[baseline=(C.base),node distance = 1cm, auto]
    \node [cloud] (C) {C};
    \node[fill=white] at (0.5,0.6) (Photo) {{\tiny }};
    \node [cloud, right of=D] (D) {D};
    \draw [->] (C) to [out=45,in=135] (D);
   \end{tikzpicture} 
   & $\begin{matrix} 1 & 1 & 2 & 2 & 2 & 2  \\ 2 & 0 & 2 & 2 & 2 & 2  \end{matrix}$ \\
   		Grim & \begin{tabular}{p{4.5cm}}Play C until either player plays D, then play D forever \end{tabular} & 
	\begin{tikzpicture}[baseline=(C.base),node distance = 1cm, auto]
    \node [cloud] (C) {C};
    \node[fill=white] at (0.5,0.6) (Photo) {{\tiny $cd,dd,dd$}};
    \node[fill=white] at (-1,0) (Photo) {{\tiny $cc$}};
    \node [cloud, right of=C] (D) {D};
    \draw [->] (C) to [out=45,in=135] (D);
    \draw [->] (C) to [out=-135,in=135,looseness=4] (C);
    \draw [->] (D) to [out=-45,in=45,looseness=4] (D);
   \end{tikzpicture} 
   & $\begin{matrix} 1 & 1 & 1 & 2 & 2 & 2  \\ 2 & 0 & 2 & 2 & 2 & 2  \end{matrix}$ \\
		TFT & \begin{tabular}{p{4.5cm}} Play C unless partner played D in previous period \end{tabular} & 
	\begin{tikzpicture}[baseline=(C.base),node distance = 1cm, auto]
    \node [cloud] (C) {C};
    \node[fill=white] at (0.5,0.6) (Photo) {{\tiny $cd,dd$}};
    \node[fill=white] at (0.5,-0.6) (Photo) {{\tiny $cc,dc$}};
    \node[fill=white] at (-1,0.14) (Photo) {{\tiny $cc,$}};
    \node[fill=white] at (-1,-0.14) (Photo) {{\tiny $dc$}};
    \node[fill=white] at (2,0.14) (Photo) {{\tiny $cd,$}};
    \node[fill=white] at (2,-0.14) (Photo) {{\tiny $dd$}};
    \node [cloud, right of=C] (D) {D};
    \draw [->] (C) to [out=45,in=135] (D);
    \draw [->] (C) to [out=-135,in=135,looseness=4] (C);
    \draw [->] (D) to [out=-45,in=45,looseness=4] (D);
   \draw [->] (D) to [out=-135,in=-45] (C);
   \end{tikzpicture}  
   & $\begin{matrix} 1 & 1 & 1 & 2 & 1 & 2  \\ 2 & 0 & 1 & 2 & 1 & 2  \end{matrix}$ \\
   			PTFT (WSLS) & \begin{tabular}{p{4.5cm}}Play C if both players chose the same move in the previous period, otherwise play D  \end{tabular} & 
	\begin{tikzpicture}[baseline=(C.base),node distance = 1cm, auto]
    \node [cloud] (C) {C};
    \node[fill=white] at (0.5,0.6) (Photo) {{\tiny $cd,dc$}};
    \node[fill=white] at (0.5,-0.6) (Photo) {{\tiny $cc,dd$}};
    \node[fill=white] at (-1,0.14) (Photo) {{\tiny $cc,$}};
    \node[fill=white] at (-1,-0.14) (Photo) {{\tiny $dd$}};
        \node[fill=white] at (2,0.14) (Photo) {{\tiny $cd,$}};
    \node[fill=white] at (2,-0.14) (Photo) {{\tiny $dc$}};
    \node [cloud, right of=C] (D) {D};
    \draw [->] (C) to [out=45,in=135] (D);
    \draw [->] (C) to [out=-135,in=135,looseness=4] (C);
    \draw [->] (D) to [out=-45,in=45,looseness=4] (D);
        \draw [->] (D) to [out=-135,in=-45] (C);
   \end{tikzpicture} 
   & $\begin{matrix} 1 & 1 & 1 & 2 & 2 & 1  \\ 2 & 0 & 1 & 2 & 2 & 1  \end{matrix}$ \\
		T2 & \begin{tabular}{p{4.5cm}}Play C until either player plays D, then play D twice and return to C (regardless of all actions during the punishment periods) \end{tabular} & 
	\begin{tikzpicture}[baseline=(C.base),node distance = 1cm, auto]
    \node [cloud] (C) {C};
    \node[fill=white] at (0.5,0.6) (Photo) {{\tiny $cd,dd,dd$}};
    \node[fill=white] at (-1,0) (Photo) {{\tiny $cc$}};
        \node [cloud, right of=C] (D1) {D};
    \node [cloud, right of=D1] (D2) {D};
    \draw [->] (C) to [out=45,in=135] (D);
    \draw [->] (C) to [out=-135,in=135,looseness=4] (C);
    \draw [->] (D1) to [out=45,in=135] (D2);
    \draw [->] (D2) to [out=225,in=315] (C);
   \end{tikzpicture}  
   & $\begin{matrix} 1 & 1 & 1 & 2 & 2 & 2  \\ 2 & 0 & 3 & 3 & 3 & 3 \\ 3 & 0 & 1 & 1 & 1 & 1 \end{matrix}$ \\
   		TF2T & \begin{tabular}{p{4.5cm}}Play C unless partner played D in both of the last 2 periods \end{tabular} & 
	\begin{tikzpicture}[baseline=(C.base),node distance = 1cm, auto]
    \node [cloud] (C) {C};
    \node[fill=white] at (-2,0.14) (Photo) {{\tiny $cc,$}};
    \node[fill=white] at (-2,-0.14) (Photo) {{\tiny $dc$}};
    \node[fill=white] at (0.5,0.6) (Photo) {{\tiny $cd,dd$}};
    \node[fill=white] at (-0.5,-0.6) (Photo) {{\tiny $cd,dd$}};
    \node[fill=white] at (0,-1) (Photo) {{\tiny $cc,dc$}};
    \node[fill=white] at (-0.5,0.6) (Photo) {{\tiny $cc,dc$}};
    \node[fill=white] at (-1,-0.14) (Photo) {{\tiny $dc$}};
    \node[fill=white] at (2,0.14) (Photo) {{\tiny $cd,$}};
    \node[fill=white] at (2,-0.14) (Photo) {{\tiny $dd$}};
    \node [cloud, left of=C] (C1) {C};
    \node [cloud, right of=C] (D) {D};
    \draw [->] (C) to [out=45,in=135] (D);
    \draw [->] (D) to [out=-45,in=45,looseness=4] (D);
    \draw [->] (C1) to [out=225,in=135,looseness=4] (C1);
    \draw [->] (C) to [out=135,in=45] (C1);
    \draw [->] (C1) to [out=315,in=225] (C);
    \draw [->] (D) to [out=-135,in=-90,looseness=1.1] (C1);
   \end{tikzpicture}     & $\begin{matrix} 1 & 1 & 1 & 2 & 1 & 2  \\ 2 & 1 & 1 & 3 & 1 & 3 \\ 3 & 0 & 1 & 3 & 1 & 3  \end{matrix}$ \\
   		TF3T & \begin{tabular}{p{4.5cm}}Play C unless partner played D in all of the last 3 periods \end{tabular} & 	
    \begin{tikzpicture}[baseline=(C.base),node distance = 1cm, auto]
    \node [cloud] (C) {C};
    \node[fill=white] at (-2,0.14) (Photo) {{\tiny $cc,$}};
    \node[fill=white] at (0,1.15) (Photo) {{\tiny $cc,dc$}};
    \node[fill=white] at (-2,-0.14) (Photo) {{\tiny $dc$}};
    \node[fill=white] at (0.5,0.6) (Photo) {{\tiny $cd,dd$}};
    \node[fill=white] at (1.5,-0.6) (Photo) {{\tiny $cd,dd$}};
    \node[fill=white] at (-0.5,-0.6) (Photo) {{\tiny $cd,dd$}};
    \node[fill=white] at (0,-1) (Photo) {{\tiny $cc,dc$}};
    \node[fill=white] at (-0.5,0.6) (Photo) {{\tiny $cc,dc$}};
    \node[fill=white] at (-1,-0.14) (Photo) {{\tiny $dc$}};
    \node[fill=white] at (3,0.14) (Photo) {{\tiny $cd,$}};
    \node[fill=white] at (3,-0.14) (Photo) {{\tiny $dd$}};
    \node [cloud, left of=C] (C1) {C};
    \node [cloud, right of=C] (C2) {C};
    \node [cloud, right of=C2] (D) {D};
    \draw [->] (C) to [out=45,in=135] (C2);
    \draw [->] (C2) to [out=-45,in=-135] (D);
    \draw [->] (D) to [out=-45,in=45,looseness=4] (D);
    \draw [->] (C1) to [out=225,in=135,looseness=4] (C1);
    \draw [->] (D) to [out=135,in=90,looseness=0.9] (C1);
    \draw [->] (C) to [out=135,in=45] (C1);
    \draw [->] (C1) to [out=315,in=225] (C);
    \draw [->] (C2) to [out=-135,in=-90,looseness=1.1] (C1);
   \end{tikzpicture}  
   & $\begin{matrix} 1 & 1 & 1 & 2 & 1 & 2  \\ 2 & 1 & 1 & 3 & 1 & 3 \\ 3 & 1 & 1 & 4 & 1 & 4 \\ 4 & 0 & 1 & 4 & 1 & 4  \end{matrix}$ \\
   \hline
\hline\\
	\end{tabular}
\end{footnotesize}
\end{table}

\begin{table}[htbp]
\caption{Pre-programmed prisoner's dilemma strategies (11-18)}
\label{tab:PD_set2}
\setlength{\tabcolsep}{5pt}
	\renewcommand\arraystretch{1}
	\centering
%	\caption{Set of strategies adopted from Fudenberg et al.}
\begin{footnotesize}
	\begin{tabular}{p{1.5cm}p{4.5cm}cc}
							\hline
						\hline\\[-1.5ex]
		{\small Acronym} & {\small Description } & {\small Automaton} & {\small  Matrix} \\[0.5ex]
			& & & $~s ~~~ \pi^{c} ~ cc ~~ cd ~~ dc ~~ dd$\\ 
%		& & & {\tiny $\begin{matrix} s & \pi^{c}_{s} & \phi_{cc} & \phi_{cd} & \phi_{dc} & \phi_{dd} \end{matrix}$ } \\
		\hline\\ [-1.2ex] 
		T2FT & \begin{tabular}{p{4.5cm}}Play C unless partner played D in either of the last 2 periods (2 periods of punishment if partner plays D) \end{tabular} & 	
	\begin{tikzpicture}[baseline=(C.base),node distance = 1cm, auto]
    \node [cloud] (D) {D};
    \node[fill=white] at (-2,0.14) (Photo) {{\tiny $cc,$}};
    \node[fill=white] at (-2,-0.14) (Photo) {{\tiny $dc$}};
    \node[fill=white] at (0.5,0.6) (Photo) {{\tiny $cc,dc$}};
    \node[fill=white] at (-0.5,-0.6) (Photo) {{\tiny $cd,dd$}};
    \node[fill=white] at (0.5,-0.6) (Photo) {{\tiny $cd,dd$}};
    \node[fill=white] at (0,-1.2) (Photo) {{\tiny $cc,dc$}};
    \node[fill=white] at (-0.5,0.7) (Photo) {{\tiny $cd,dd$}};
    \node[fill=white] at (-1,-0.14) (Photo) {{\tiny $dc$}};
    \node [cloud, left of=C] (C) {C};
    \node [cloud, right of=D] (D2) {D};
    \draw [->] (D) to [out=180,in=115,looseness=4] (D);
    \draw [->] (D1) to [out=225,in=315] (D);
    \draw [->] (C) to [out=225,in=135,looseness=4] (C);
    \draw [->] (D) to [out=45,in=135] (D1);
    \draw [->] (C) to [out=315,in=225] (D);
    \draw [->] (D1) to [out=270,in=-90,looseness=1] (C);
   \end{tikzpicture}     & $\begin{matrix} 1 & 1 & 1 & 2 & 1 & 2  \\ 2 & 0 & 3 & 2 & 3 & 2 \\ 3 & 0 & 1 & 2 & 1 & 2  \end{matrix}$ \\
		T2F2T & \begin{tabular}{p{4.5cm}}Play C unless partner played 2 consecutive Ds in the last 3 periods (2 periods of punishment if partner plays D twice in a row) \end{tabular}  &  	
	\begin{tikzpicture}[baseline=(C.base),node distance = 1cm, auto]
    \node [cloud] (D) {D};
    \node[fill=white] at (-3,0.14) (Photo) {{\tiny $cc,$}};
    \node[fill=white] at (-3,-0.14) (Photo) {{\tiny $dc$}};
    \node[fill=white] at (-2,0.14) (Photo) {{\tiny $cc,$}};
    \node[fill=white] at (-2,-0.14) (Photo) {{\tiny $dc$}};
    \node[fill=white] at (0.5,0.6) (Photo) {{\tiny $cc,dc$}};
    \node[fill=white] at (-0.5,-0.6) (Photo) {{\tiny $cd,dd$}};
    \node[fill=white] at (0.5,-0.6) (Photo) {{\tiny $cd,dd$}};
    \node[fill=white] at (-0.5,-1.1) (Photo) {{\tiny $cc,dc$}};
    \node[fill=white] at (-1.5,0.6) (Photo) {{\tiny $cd,dd$}};
    \node[fill=white] at (-1.5,-0.6) (Photo) {{\tiny $cc,dc$}};
    \node[fill=white] at (-0.5,0.7) (Photo) {{\tiny $cd,dd$}};
    \node[fill=white] at (-1,-0.14) (Photo) {{\tiny $dc$}};
    \node [cloud, left of=D] (C) {C};
    \node [cloud, left of=C] (C1) {C};
    \node [cloud, right of=D] (D2) {D};
    \draw [->] (D) to [out=180,in=115,looseness=4] (D);
    \draw [->] (D1) to [out=225,in=315] (D);
    \draw [->] (C1) to [out=225,in=135,looseness=4] (C1);
    \draw [->] (C1) to [out=45,in=135] (C);
    \draw [->] (C) to [out=225,in=315] (C1);
    \draw [->] (D) to [out=45,in=135] (D1);
    \draw [->] (C) to [out=315,in=225] (D);
    \draw [->] (D1) to [out=315,in=-90,looseness=0.7] (C1);
   \end{tikzpicture}     & $\begin{matrix} 1 & 1 & 1 & 2 & 1 & 2  \\ 2 & 1 & 1 & 3 & 1 & 3 \\ 3 & 0 & 4 & 3 & 4 & 3 \\ 4 & 0 & 1 & 3 & 1 & 3 \end{matrix}$ \\
		Grim2 & \begin{tabular}{p{4.5cm}}Play C until 2 consecutive periods occur in which either player played D, then play D forever \end{tabular} & 	
	\begin{tikzpicture}[baseline=(C.base),node distance = 1cm, auto]
    \node [cloud] (C1) {C};
    \node[fill=white] at (0.5,0.6) (Photo) {{\tiny $cc$}};
    \node[fill=white] at (1.5,0.6) (Photo) {{\tiny $cd,dd,dd$}};
    \node[fill=white] at (0.5,-0.6) (Photo) {{\tiny $cd,dd,dd$}};
    \node[fill=white] at (-1,0) (Photo) {{\tiny $cc$}};
    \node [cloud, right of=C1] (C2) {C};
    \node [cloud, right of=C2] (D) {D};
        \draw [->] (C2) to [out=135,in=45] (C1);
                \draw [->] (C1) to [out=315,in=225] (C2);
    \draw [->] (C2) to [out=45,in=135] (D);
    \draw [->] (C1) to [out=-135,in=135,looseness=4] (C1);
   \draw [->] (D) to [out=-45,in=45,looseness=4] (D);
   \end{tikzpicture} 
      & $\begin{matrix} 1 & 1 & 1 & 2 & 2 & 2  \\ 2 & 1 & 1 & 3 & 3 & 3 \\ 3 & 0 & 3 & 3 & 3 & 3  \end{matrix}$ \\
		Grim3 & \begin{tabular}{p{4.5cm}}Play C until 3 consecutive periods occur in which either player played D, then play D forever \end{tabular}  & 
	\begin{tikzpicture}[baseline=(C.base),node distance = 1cm, auto]
    \node [cloud] (C1) {C};
    \node[fill=white] at (0.5,0.6) (Photo) {{\tiny $cc$}};
    \node[fill=white] at (1.2,1.1) (Photo) {{\tiny $cc$}};
    \node[fill=white] at (1.5,0.6) (Photo) {{\tiny $cd,dd,dd$}};
    \node[fill=white] at (0.5,-0.6) (Photo) {{\tiny $cd,dd,dd$}};
        \node[fill=white] at (2.5,-0.6) (Photo) {{\tiny $cd,dd,dd$}};
    \node[fill=white] at (-1,0) (Photo) {{\tiny $cc$}};
    \node [cloud, right of=C1] (C2) {C};
    \node [cloud, right of=C2] (C3) {C};
    \node [cloud, right of=C3] (D) {D};
    \draw [->] (C2) to [out=135,in=45] (C1);
    \draw [->] (C1) to [out=315,in=225] (C2);
    \draw [->] (C2) to [out=45,in=135] (C3);
    \draw [->] (C3) to [out=45,in=90,looseness=1.1] (C1);
    \draw [->] (C3) to [out=315,in=225] (D);
    \draw [->] (C1) to [out=-135,in=135,looseness=4] (C1);
    \draw [->] (D) to [out=-45,in=45,looseness=4] (D);
   \end{tikzpicture} 
      & $\begin{matrix} 1 & 1 & 1 & 2 & 2 & 2  \\ 2 & 1 & 1 & 3 & 3 & 3 \\ 3 & 1 & 1 & 4 & 4 & 4  \\ 4 & 0 & 4 & 4 & 4 & 4 \end{matrix}$ \\ 
      		PT2FT & \begin{tabular}{p{4.5cm}}Play C if both players played C in the last 2 periods, both players played D in the last 2 periods, or both players played D 2 periods ago and C in the previous period. Otherwise play D \end{tabular}  & 
	\begin{tikzpicture}[baseline=(C.base),node distance = 1cm, auto]
    \node [cloud] (C) {D};
    \node[fill=white] at (0.5,0.6) (Photo) {{\tiny $cc,dd$}};
    \node[fill=white] at (0,-1.1) (Photo) {{\tiny $cc,dd$}};
    \node[fill=white] at (-0.5,-0.6) (Photo) {{\tiny $cd,dc$}};
    \node[fill=white] at (-0.4,0.7) (Photo) {{\tiny $cd,dc$}};
    \node[fill=white] at (0.5,-0.6) (Photo) {{\tiny $cd,dc$}};
    \node[fill=white] at (-2,0.14) (Photo) {{\tiny $cc,$}};
    \node[fill=white] at (-2,-0.14) (Photo) {{\tiny $dd$}};
    \node [cloud, left of=C] (C1) {C};
    \node [cloud, right of=C] (D) {D};
    \draw [->] (C) to [out=45,in=135] (D);
    \draw [->] (C1) to [out=315,in=225] (C);
    \draw [->] (C) to [out=180,in=115,looseness=4] (C);
    \draw [->] (C1) to [out=-135,in=135,looseness=4] (C1);
    \draw [->] (D) to [out=270,in=270,looseness=0.9] (C1);
    \draw [->] (D) to [out=-135,in=-45] (C);
   \end{tikzpicture} 
   & $\begin{matrix} 1 & 1 & 1 & 2 & 2 & 1  \\ 2 & 0 & 3 & 2 & 2 & 3  \\ 3 & 0 & 1 & 2 & 2 & 1 \end{matrix}$ \\
      DTFT & \begin{tabular}{p{4.5cm}} Play D in the first period, then play TFT \end{tabular} & 
	\begin{tikzpicture}[baseline=(C.base),node distance = 1cm, auto]
    \node [cloud] (D) {D};
    \node[fill=white] at (0.5,0.6) (Photo) {{\tiny $cc,dc$}};
    \node[fill=white] at (0.5,-0.6) (Photo) {{\tiny $cd,dd$}};
    \node[fill=white] at (-1,0.14) (Photo) {{\tiny $cd,$}};
    \node[fill=white] at (-1,-0.14) (Photo) {{\tiny $dd$}};
        \node[fill=white] at (2,0.14) (Photo) {{\tiny $cc,$}};
    \node[fill=white] at (2,-0.14) (Photo) {{\tiny $dc$}};
    \node [cloud, right of=D] (C) {C};
    \draw [->] (D) to [out=45,in=135] (C);
    \draw [->] (D) to [out=-135,in=135,looseness=4] (D);
    \draw [->] (C) to [out=-45,in=45,looseness=4] (C);
        \draw [->] (C) to [out=-135,in=-45] (D);
   \end{tikzpicture} 
   & $\begin{matrix} 1 & 0 & 2 & 1 & 2 & 1  \\ 2 & 1 & 2 & 1 & 2 & 1  \end{matrix}$ \\
		DTF2T  & \begin{tabular}{p{4.5cm}}Play D in the first period, then play TF2T  \end{tabular} &
	\begin{tikzpicture}[baseline=(C.base),node distance = 1cm, auto]
    \node [cloud] (C) {C};
    \node[fill=white] at (-2,0.14) (Photo) {{\tiny $cc,$}};
    \node[fill=white] at (-2,-0.14) (Photo) {{\tiny $dc$}};
    \node[fill=white] at (0.5,0.6) (Photo) {{\tiny $cd,dd$}};
    \node[fill=white] at (-0.5,-0.6) (Photo) {{\tiny $cd,dd$}};
    \node[fill=white] at (0,-1) (Photo) {{\tiny $cc,dc$}};
    \node[fill=white] at (-0.5,0.6) (Photo) {{\tiny $cc,dc$}};
    \node[fill=white] at (-1,-0.14) (Photo) {{\tiny $dc$}};
    \node[fill=white] at (2,0.14) (Photo) {{\tiny $cd,$}};
    \node[fill=white] at (2,-0.14) (Photo) {{\tiny $dd$}};
    \node[fill=white] at (-1.5,-0.6) (Photo) {{\tiny $cc,dc$}};
    \node[fill=white] at (-1.5,0.7) (Photo) {{\tiny $cc,dc$}};
    \node[fill=white] at (-1.1,1.25) (Photo) {{\tiny $cd,dd$}};
    \node [cloud, left of=C] (C1) {C};
    \node [cloud, right of=C] (D) {D};
    \node [cloud, left of=C1] (D1) {D};
    \draw [->] (C) to [out=45,in=135] (D);
   	\draw [->] (D1) to [out=110,in=90,looseness=1.2] (C);
    \draw [->] (D) to [out=-45,in=45,looseness=4] (D);
    \draw [->] (C1) to [out=180,in=115,looseness=4] (C1);
    \draw [->] (D1) to [out=315,in=225] (C1);
    \draw [->] (C) to [out=135,in=45] (C1);
    \draw [->] (C1) to [out=315,in=225] (C);
    \draw [->] (D) to [out=-135,in=-90,looseness=1.1] (C1);
   \end{tikzpicture}     
   & $\begin{matrix} 1 & 0 & 2 & 3 & 2 & 3  \\ 2 & 1 & 2 & 3 & 2 & 3 \\ 3 & 1 & 2 & 4 & 2 & 4 \\ 4 & 0 & 2 & 4 & 2 & 4   \end{matrix}$ \\
		DTF3T  & \begin{tabular}{p{4.5cm}}Play D in the first period, then play TF3T \end{tabular} &     
    \begin{tikzpicture}[baseline=(C.base),node distance = 1cm, auto]
    \node [cloud] (C) {C};
    \node[fill=white] at (-2,0.14) (Photo) {{\tiny $cc,$}};
    \node[fill=white] at (0,1.15) (Photo) {{\tiny $cc,dc$}};
    \node[fill=white] at (-2,-0.14) (Photo) {{\tiny $dc$}};
    \node[fill=white] at (0.5,0.6) (Photo) {{\tiny $cd,dd$}};
    \node[fill=white] at (1.5,-0.6) (Photo) {{\tiny $cd,dd$}};
    \node[fill=white] at (-0.5,-0.6) (Photo) {{\tiny $cd,dd$}};
    \node[fill=white] at (0,-1) (Photo) {{\tiny $cc,dc$}};
    \node[fill=white] at (-0.5,0.6) (Photo) {{\tiny $cc,dc$}};
    \node[fill=white] at (-1,-0.14) (Photo) {{\tiny $dc$}};
    \node[fill=white] at (3,0.14) (Photo) {{\tiny $cd,$}};
    \node[fill=white] at (3,-0.14) (Photo) {{\tiny $dd$}};
    \node[fill=white] at (-1.5,0.6) (Photo) {{\tiny $cc,dc$}};
    \node[fill=white] at (-1.5,-0.6) (Photo) {{\tiny $cc,dc$}};
    \node[fill=white] at (-1.1,1.25) (Photo) {{\tiny $cd,dd$}};
    \node [cloud, left of=C] (C1) {C};
    \node [cloud, right of=C] (C2) {C};
    \node [cloud, right of=C2] (D) {D};
    \node [cloud, left of=C1] (D1) {D};
    \draw [->] (C) to [out=45,in=135] (C2);
    \draw [->] (D1) to [out=315,in=225] (C1);
    \draw [->] (D1) to [out=110,in=90,looseness=1.2] (C);
    \draw [->] (C2) to [out=-45,in=-135] (D);
    \draw [->] (D) to [out=-45,in=45,looseness=4] (D);
        \draw [->] (C1) to [out=180,in=115,looseness=4] (C1);
    \draw [->] (D) to [out=135,in=90,looseness=0.9] (C1);
    \draw [->] (C) to [out=135,in=45] (C1);
    \draw [->] (C1) to [out=315,in=225] (C);
    \draw [->] (C2) to [out=-135,in=-90,looseness=1.1] (C1);
   \end{tikzpicture}  
   & $\begin{matrix} 1 & 0 & 2 & 3 &2 & 3  \\ 2 & 1 & 2 & 3 & 2 & 3 \\ 3 & 1 & 2 & 4 & 2 & 4 \\ 4 & 1 & 2 & 5 & 2 & 5 \\ 5 & 0 & 2 & 5 & 2 & 5  \end{matrix}$ \\
         \hline
         \hline\\
	\end{tabular}
\end{footnotesize}
\end{table}

\begin{table}[htbp]
\caption{Pre-programmed prisoner's dilemma strategies (19-22)}
\label{tab:PD_set3}
\setlength{\tabcolsep}{5pt}
	\renewcommand\arraystretch{1}
	\centering
%	\caption{Set of strategies adopted from Fudenberg et al.}
\begin{footnotesize}
	\begin{tabular}{p{1.5cm}p{4.5cm}cc}
							\hline
						\hline\\[-1.5ex]
		{\small Acronym} & {\small Description } & {\small Automaton} & {\small  Matrix} \\[0.5ex] 
			& & & $~s ~~~ \pi^{c} ~ cc ~~ cd ~~ dc ~~ dd$\\
%		& & & {\tiny $\begin{matrix} s & \pi^{c}_{s} & \phi_{cc} & \phi_{cd} & \phi_{dc} & \phi_{dd} \end{matrix}$ } \\
		\hline\\ [-1.2ex] 
				DGrim2 & \begin{tabular}{p{4.5cm}}Play D in the first period, then play Grim2 \end{tabular} & 	
	\begin{tikzpicture}[baseline=(C.base),node distance = 1cm, auto]
    \node [cloud] (C1) {C};
    \node[fill=white] at (0.5,0.6) (Photo) {{\tiny $cc$}};
    \node[fill=white] at (0,0.8) (Photo) {{\tiny $cc$}};
    \node[fill=white] at (-0.5,0.6) (Photo) {{\tiny $cc$}};
    \node[fill=white] at (1.5,0.6) (Photo) {{\tiny $cd,dd,dd$}};
    \node[fill=white] at (0.5,-0.6) (Photo) {{\tiny $cd,dd,dd$}};
    \node[fill=white] at (0,-1.1) (Photo) {{\tiny $cd,dd,dd$}};
    \node[fill=white] at (-1,0) (Photo) {{\tiny $cc$}};
    \node [cloud, left of=C1] (D1) {D};
    \node [cloud, right of=C1] (C2) {C};
    \node [cloud, right of=C2] (D) {D};
        \draw [->] (C2) to [out=135,in=45] (C1);
    \draw [->] (C1) to [out=315,in=225] (C2);
    \draw [->] (C2) to [out=45,in=135] (D);
    \draw [->] (C1) to [out=115,in=65,looseness=4] (C1);
    \draw [->] (D1) to [out=45,in=135] (C1);
    \draw [->] (D1) to [out=270,in=315] (C2);
    \draw [->] (D) to [out=-45,in=45,looseness=4] (D);
   \end{tikzpicture} 
      & $\begin{matrix} 1 & 0 & 2 & 3 & 3 & 3  \\ 2 & 1 & 2 & 3 & 3 & 3 \\ 3 & 1 & 2 & 4 & 4 & 4 \\ 4 & 0 & 4 & 4 & 4 & 4  \end{matrix}$ \\
     DGrim3 & \begin{tabular}{p{4.5cm}}Play D in the first periodperiod, then play Grim3 \end{tabular} & 
	\begin{tikzpicture}[baseline=(C.base),node distance = 1cm, auto]
    \node [cloud] (C1) {C};
    \node[fill=white] at (0.5,0.6) (Photo) {{\tiny $cc$}};
    \node[fill=white] at (-0.5,-0.6) (Photo) {{\tiny $cc$}};
    \node[fill=white] at (-0.4,0.7) (Photo) {{\tiny $cc$}};
    \node[fill=white] at (1.2,1.1) (Photo) {{\tiny $cc$}};
    \node[fill=white] at (1.5,0.6) (Photo) {{\tiny $cd,dd,dd$}};
    \node[fill=white] at (0.5,-0.6) (Photo) {{\tiny $cd,dd,dd$}};
    \node[fill=white] at (0,-1.1) (Photo) {{\tiny $cd,dd,dd$}};
    \node[fill=white] at (2.5,-0.6) (Photo) {{\tiny $cd,dd,dd$}};
    \node[fill=white] at (-1,0) (Photo) {{\tiny $cc$}};
    \node [cloud, left of=C1] (D1) {D};
    \node [cloud, right of=C1] (C2) {C};
    \node [cloud, right of=C2] (C3) {C};
    \node [cloud, right of=C3] (D) {D};
    \draw [->] (C2) to [out=135,in=45] (C1);
    \draw [->] (D1) to [out=315,in=225] (C1);
    \draw [->] (C1) to [out=315,in=225] (C2);
    \draw [->] (C2) to [out=45,in=135] (C3);
    \draw [->] (C3) to [out=45,in=90,looseness=1.1] (C1);
    \draw [->] (C3) to [out=315,in=225] (D);
    \draw [->] (C1) to [out=180,in=115,looseness=4] (C1);
    \draw [->] (D1) to [out=270,in=315] (C2);
    \draw [->] (D) to [out=-45,in=45,looseness=4] (D);
   \end{tikzpicture} 
      & $\begin{matrix} 1 & 0 & 2 & 3 & 3 & 3  \\ 2 & 1 & 2 & 3 & 3 & 3 \\ 3 & 1 & 2 & 4 & 4 & 4  \\ 4 & 1 & 2 & 5 & 5 & 5 \\ 5 & 0 & 5 & 5 & 5 & 5 \end{matrix}$ \\
		SGrim & \begin{tabular}{p{4.5cm}}Play C if both players played C, and D if both players played D. If one player played D and the other C, play C with probability $\alpha$. \end{tabular} & 
	\begin{tikzpicture}[baseline=(C.base),node distance = 1cm, auto]
    \node [cloud] (C) {C};
    \node[fill=white] at (1,-0) (Photo) {{\tiny $dd$}};
    \node[fill=white] at (0,1) (Photo) {{\tiny $cc$}};
    \node[fill=white] at (2,1) (Photo) {{\tiny $dd$}};
    \node[fill=white] at (0.6,0.5) (Photo) {{\tiny $cd,dc$}};
    \node[fill=white] at (1.4,0.5) (Photo) {{\tiny $cd,dc$}};
    \node[fill=white] at (1,-0.4) (Photo) {{\tiny $cc$}};
    \node[fill=white] at (-1,0) (Photo) {{\tiny $cc$}};
    \node[fill=white] at (1,1.9) (Photo) {{\tiny $cd,dc$}};
    \node[fill=white] at (3,0) (Photo) {{\tiny $dd$}};
    \node [, right of=C] (Z) {};
    \node [cloud, above of=Z] (M2) {$\alpha$};
    \node [cloud, right of=Z] (D) {D};
    \draw [->] (C) to  [out=340,in=200,looseness=0.9] (D);
    \draw [->] (D) to  [out=225,in=315] (C);
    \draw [->] (C) to [out=80,in=190,looseness=1] (M2);
    \draw [->] (M2) to [out=170,in=100,looseness=1] (C);
    \draw [->] (M2) to [out=10,in=80,looseness=1] (D);
    \draw [->] (D) to [out=100,in=350,looseness=1] (M2);
    \draw [->] (M2) to [out=135,in=45,looseness=4] (M2);
    \draw [->] (C) to [out=225,in=135,looseness=4] (C);
    \draw [->] (D) to [out=315,in=45,looseness=4] (D);
   \end{tikzpicture} 
   & $\begin{matrix} 1 & 1 & 1 & 2 & 2 & 3  \\ 2 & \text{NA} & 1 & 2 & 2 & 3  \\ 3 & 0 & 1 & 2 & 2 & 3  \end{matrix}$ \\
M1BF & \begin{tabular}{p{4.5cm}}Play C if both players played C, and D if both players played D. If the own action was C and the other player played D, play C with probability $\alpha$. If the own action was D and the other player played C, play C with probability $\beta$.  \end{tabular} & 
	\begin{tikzpicture}[baseline=(C.base),node distance = 1cm, auto]
    \node [cloud] (C) {C};
    \node[fill=white] at (1,0.4) (Photo) {{\tiny $dd$}};
    \node[fill=white] at (0,1) (Photo) {{\tiny $cd$}};
    \node[fill=white] at (2,1) (Photo) {{\tiny $dd$}};
    \node[fill=white] at (2,-1) (Photo) {{\tiny $dc$}};
    \node[fill=white] at (0,-1) (Photo) {{\tiny $cc$}};      
    \node[fill=white] at (0.4,0.6) (Photo) {{\tiny $cc$}};
    \node[fill=white] at (1.6,0.6) (Photo) {{\tiny $cd$}};
    \node[fill=white] at (1.6,-0.6) (Photo) {{\tiny $dd$}};
    \node[fill=white] at (0.4,-0.6) (Photo) {{\tiny $dc$}}; 
    \node[fill=white] at (1,-0.4) (Photo) {{\tiny $cc$}};
    \node[fill=white] at (0.6,0) (Photo) {{\tiny $cd$}};
    \node[fill=white] at (1.4,0) (Photo) {{\tiny $dc$}};
    \node[fill=white] at (-0.9,0) (Photo) {{\tiny $cc$}};
    \node[fill=white] at (1,1.8) (Photo) {{\tiny $cd$}};
    \node[fill=white] at (1,-1.8) (Photo) {{\tiny $dc$}};
    \node[fill=white] at (2.9,0) (Photo) {{\tiny $dd$}};
    \node [, right of=C] (Z) {};
    \node [cloud, below of=Z] (M1) {$\beta$};
    \node [cloud, above of=Z] (M2) {$\alpha$};
    \node [cloud, right of=Z] (D) {D};
    \draw [->] (C) to [out=45,in=135] (D);
    \draw [->] (D) to [out=225,in=315] (C);
    \draw [->] (M1) to [out=135,in=225] (M2);
    \draw [->] (M2) to [out=315,in=45] (M1);
    \draw [->] (C) to [out=100,in=170,looseness=0.9] (M2);
    \draw [->] (M2) to [out=190,in=80,looseness=1] (C);
    \draw [->] (M2) to [out=10,in=80,looseness=0.9] (D);
    \draw [->] (D) to [out=100,in=350,looseness=1] (M2);
    \draw [->] (D) to [out=280,in=350,looseness=0.9] (M1);
    \draw [->] (M1) to [out=10,in=260,looseness=1] (D);
    \draw [->] (M1) to [out=190,in=260,looseness=0.9] (C);
    \draw [->] (C) to [out=280,in=170,looseness=1] (M1);
    \draw [->] (M1) to [out=225,in=315,looseness=4] (M1);
    \draw [->] (M2) to [out=135,in=45,looseness=4] (M2);
    \draw [->] (C) to [out=225,in=135,looseness=4] (C);
    \draw [->] (D) to [out=315,in=45,looseness=4] (D);
   \end{tikzpicture} 
   & $\begin{matrix} 1 & 1 & 1 & 2 & 3 & 4  \\ 2 & \text{NA} & 1 & 2 & 3 & 4  \\ 3 & \text{NA} & 1 & 2 & 3 & 4 \\ 4 & 0 & 1 & 2 & 3 & 4  \end{matrix}$ \\
		\hline
		\hline\\
	\end{tabular}
\end{footnotesize}
\end{table}  

\end{document}
